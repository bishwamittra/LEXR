{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from RNN2DFA.LSTM import LSTMNetwork\n",
    "# from GRU import GRUNetwork\n",
    "from RNN2DFA.RNNClassifier import RNNClassifier\n",
    "from RNN2DFA.Training_Functions import mixed_curriculum_train\n",
    "from RNN2DFA.Training_Functions import make_test_set,make_train_set_for_target\n",
    "from lexr.RNNexplainer import Explainer\n",
    "import pandas as pd\n",
    "import lexr.LTL2DFA as ltlf2dfa\n",
    "from RNN2DFA.Extraction import extract\n",
    "import pickle\n",
    "from samples2ltl.utils.Traces import Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reber grammar\n",
    "from lexr.specific_examples import Reber_Grammar\n",
    "rg=Reber_Grammar()\n",
    "alphabet=rg.alphabet\n",
    "generator_dfa=rg\n",
    "sample_train_set=[]\n",
    "for i in range(100):\n",
    "    seq, _, _ = rg.get_one_example(maxLength=10)\n",
    "    sample_train_set.append(seq)\n",
    "    # print(rg.classify_word(rg.sequenceToWord(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['|(x192,F(x101))', '|(x341,|(x49,x797))', '|(x40,F(x19))', '|(x591,F(x5))', '|(x528,F(x169))', '|(x100,F(x4))', '|(x172,F(x2))', '|(x482,F(x124))', '|(x47,x736)', '|(x47,x720)', '|(x124,F(x19))', 'F(|(x15,x21))', 'F(|(x323,x4))', '|(x797,F(x26))', 'F(x912)', 'U(!(x3),x2)', '|(x113,F(x19))', '|(x192,F(x19))', 'F(|(x148,x71))', '|(x188,F(x21))', '|(x235,F(x48))', '|(x16,F(x4))', 'F(x26)', '|(x298,|(x210,x3))', 'F(x4)', '|(x3,F(x21))', '|(x70,F(x12))', '|(x100,F(x3))', '|(x324,|(x2,x420))', 'F(x5)', '|(x235,F(x650))', 'F(|(x106,x21))', '|(x13,F(x96))', '|(x488,F(x9))', 'F(x21)', 'F(|(x122,x31))', '|(x889,F(x35))', '|(x22,F(x12))', '|(x28,x438)', 'F(|(x2,x958))', '|(x344,x47)', 'F(x197)', '|(x192,x583)', '|(x73,F(x7))', '|(x641,F(x2))', '|(x178,F(x2))', 'F(x96)', '|(x637,F(x47))', 'F(x388)', '|(x773,F(x26))', 'F(x38)', '|(x83,F(x873))', 'F(|(x147,x47))', '|(x192,x483)', '|(x104,F(x337))', '|(x13,F(x9))', '|(x113,F(x16))', '|(x113,F(x3))', '|(x70,F(x96))', '|(x175,F(x19))', '|(x106,F(x22))', '|(x7,|(x244,x3))', 'F(|(x124,x210))', '|(x21,x39)', '|(x114,F(x341))', '|(x136,x48)', 'F(|(x124,x252))', 'F(x12)', 'F(x84)', '|(x12,F(x42))', 'F(|(x416,x84))', 'F(x73)', 'U(!(x118),x4)', 'F(|(x48,x73))', 'X(|(x13,x22))', '|(x175,F(x3))', '|(x121,F(x5))', '|(x797,F(x13))', 'U(!(x1),x2)', '|(x848,X(x13))', '|(x495,F(x96))', 'F(x31)', '|(x27,x35)', '|(x148,F(x21))', 'F(x2)', '|(x6,F(x15))', 'F(|(x124,x696))', 'F(|(x40,x84))', '|(x244,F(x47))', '|(x483,F(x13))', '|(x263,F(x47))', 'F(x8)', '|(x47,x773)', 'F(x68)', '|(x222,x39)', 'F(x36)', '|(x1,x94)', 'U(!(x495),x2)', '|(x192,x7)', '|(x35,F(x16))', '|(x703,F(x42))', 'G(!(x1))', '|(x62,F(x2))', 'F(x124)', '|(x756,F(x12))', '|(x222,F(x31))', '|(x47,F(x26))', 'F(|(x33,x73))', '|(x106,F(x2))', 'F(x13)', '|(x242,F(x5))', 'F(x7)', '|(x3,F(x141))', '|(x1,F(x16))', '|(x378,F(x2))', 'F(x16)', '|(x226,F(x96))', 'F(|(x17,x242))', 'F(x47)', '|(x289,F(x16))', '|(x113,x3)', '|(x127,F(x620))', 'U(!(x53),x2)', '|(x27,F(x119))', 'F(|(x16,x227))', '|(x342,F(x101))', '!(F(x1))', '|(x323,F(x16))', '|(x47,F(x2))', 'F(|(x230,x3))', 'F(|(x13,x295))', 'F(|(x113,x6))', '|(x178,x3)', '|(x12,F(x591))', '|(x172,|(x495,x504))', '|(x113,x178)', '|(x719,F(x501))', 'F(x6)', '|(x123,|(x4,x538))', 'F(x3)', 'F(|(x16,x462))', 'F(|(x105,x47))', '|(x40,F(x35))', 'F(x42)', 'F(|(x368,x84))', '|(x179,F(x42))', 'F(|(x48,x960))', '->(F(x9),x40)', '|(x48,F(x13))', '|(x12,F(x18))', 'F(|(x12,x21))', '|(x12,F(x2))', 'F(|(x73,x86))', '|(x3,F(x39))', '|(x257,x53)', 'F(x19)', '|(x12,F(x16))', 'F(x92)']\nText classification\n"
    }
   ],
   "source": [
    "import lexr.specific_examples\n",
    "generator_dfa=lexr.specific_examples.Text_Classification()\n",
    "target_formula = generator_dfa.target_formula\n",
    "alphabet = generator_dfa.alphabet\n",
    "query_formulas = generator_dfa.query_formulas\n",
    "print(target_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loading from previously stored benchmarks\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def dict2lists(dictionary):\n",
    "    X,y=[],[]\n",
    "    for key in dictionary:\n",
    "        X.append(key)\n",
    "        y.append(dictionary[key])\n",
    "    return X,y\n",
    "\n",
    "def lists2dict(x,y):\n",
    "    # both x and y should have same length\n",
    "    assert len(x)==len(y), \"Error dimension\"\n",
    "    d={}\n",
    "    n=len(x)\n",
    "    for idx in range(n):\n",
    "        d[x[idx]]=y[idx]\n",
    "    return d\n",
    "\n",
    "maximum_sequence_length = 50\n",
    "num_layers = 2\n",
    "num_hidden_dim = 10\n",
    "input_dim = 3\n",
    "iterations = 1\n",
    "stop_threshold = 0.0005\n",
    "RNNClass = LSTMNetwork\n",
    "if(target_formula == \"DNA sequence\"):\n",
    "    num_layers = 5\n",
    "    num_hidden_dim = 10\n",
    "    input_dim = 5\n",
    "    stop_threshold = 0.1\n",
    "    RNNClass = LSTMNetwork\n",
    "\n",
    "if(target_formula == \"Text classification\" or target_formula == \"Deceptive opinion\"):\n",
    "    num_layers = 5\n",
    "    num_hidden_dim = 20\n",
    "    input_dim = 10\n",
    "    stop_threshold = 0.1\n",
    "    RNNClass = LSTMNetwork\n",
    "\n",
    "\n",
    "# for each example, specify a different generating function\n",
    "file_name = \"benchmarks/\" + target_formula.replace(\" \", \"_\")+\".pkl\"\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "\n",
    "    if(target_formula == \"balanced parentheses\"):\n",
    "\n",
    "        train_set = generator_dfa.get_balanced_parantheses_train_set(8000, 2, 50, max_train_samples_per_length=3000,\n",
    "                                                                     search_size_per_length=2000, lengths=[i for i in range(maximum_sequence_length+1)])\n",
    "\n",
    "    elif(target_formula == \"email match\"):\n",
    "\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=1000,\n",
    "                                              search_size_per_length=3000, deviation=200)\n",
    "\n",
    "        # generate more examples that match the regular expression\n",
    "        matching_strings = generator_dfa.generate_matching_strings(\n",
    "            n=10800, max_length=50)\n",
    "        for string in matching_strings:\n",
    "            train_set[string] = True\n",
    "\n",
    "    elif(target_formula == \"alternating bit protocol\"):\n",
    "\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=1000,\n",
    "                                              search_size_per_length=3000, deviation=250)\n",
    "\n",
    "        # generate more examples that match the regular expression\n",
    "        matching_strings = generator_dfa.generate_matching_strings(\n",
    "            n=105000, max_sequence_length=50)\n",
    "        for string in matching_strings:\n",
    "            train_set[string] = True\n",
    "    elif(target_formula == 'G(a->X(b))'):\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=1000,\n",
    "                                              search_size_per_length=3000, deviation=20)\n",
    "    elif(target_formula == \"DNA sequence\" or target_formula == \"Text classification\" or target_formula == \"Deceptive opinion\"):\n",
    "        train_set = generator_dfa.dict\n",
    "        \n",
    "    else:\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=100,\n",
    "                                              search_size_per_length=300, deviation=20)\n",
    "\n",
    "    # now save the dataset to file\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(train_set, f, protocol=3)\n",
    "\n",
    "else:\n",
    "    # load the dataset\n",
    "    print(\"loading from previously stored benchmarks\")\n",
    "\n",
    "    def load_obj(name):\n",
    "        with open(name, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    train_set = load_obj(file_name)\n",
    "\n",
    "\n",
    "# print(train_set)\n",
    "# print(generator_dfa.alphabet)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Text classification\nout of  5043  sequences 574  are positive. (percent:  0.11382113821138211 )\nexamples per length: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nEmpty string is not in test test also\nsize of train set: 4034\nsize of test set: 1009\nconfigurations: layers:  5 hidden dimension:  20 input dim:  10 network:  <class 'RNN2DFA.LSTM.LSTMNetwork'> stop threshold:  0.1\nThe dy.parameter(...) call is now DEPRECATED.\n        There is no longer need to explicitly add parameters to the computation graph.\n        Any used parameter will be added automatically.\nloading already saved model\ntesting on train set, i.e. test set is train set\nrnn score against target on test set:                              986 (97.72)\n"
    }
   ],
   "source": [
    "print(target_formula)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print ratio\n",
    "cnt = 0\n",
    "examples_per_length = [0 for i in range(51)]\n",
    "for key in train_set:\n",
    "    if(train_set[key]):\n",
    "        cnt += 1\n",
    "    try:\n",
    "        if(\"x\" in key):\n",
    "            examples_per_length[len(key.split('x')[1:])] += 1\n",
    "        else:\n",
    "            examples_per_length[len(key)] += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "total_samples = len(train_set)\n",
    "print(\"out of \", total_samples, \" sequences\", cnt,\n",
    "      \" are positive. (percent: \", float(cnt/total_samples), \")\")\n",
    "print(\"examples per length:\", examples_per_length)\n",
    "\n",
    "# split train:test\n",
    "X, y = dict2lists(train_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "train_set = lists2dict(X_train, y_train)\n",
    "test_set = lists2dict(X_test, y_test)\n",
    "train_set_size = len(train_set)\n",
    "test_set_size = len(test_set)\n",
    "\n",
    "# intentionally pushing \"\" (empty string) in train_set\n",
    "if('' not in train_set):\n",
    "    try:\n",
    "        train_set[''] = test_set['']\n",
    "        print(\"Empty string status:\", train_set[''])\n",
    "    except:\n",
    "        print(\"Empty string is not in test test also\")\n",
    "else:\n",
    "    print(\"Empty string was already included in train set\")\n",
    "    print(\"Empty string status:\", train_set[''])\n",
    "\n",
    "print(\"size of train set:\", train_set_size)\n",
    "print(\"size of test set:\", test_set_size)\n",
    "\n",
    "print(\"configurations: layers: \", num_layers,\n",
    "          \"hidden dimension: \", num_hidden_dim,\n",
    "          \"input dim: \", input_dim,\n",
    "          \"network: \", RNNClass,\n",
    "          \"stop threshold: \", stop_threshold)\n",
    "\n",
    "\n",
    "# define rnn\n",
    "rnn = RNNClassifier(alphabet, num_layers=num_layers,\n",
    "                    hidden_dim=num_hidden_dim, RNNClass=RNNClass, input_dim=input_dim, target=target_formula)\n",
    "\n",
    "# train the model\n",
    "if not os.path.isfile(\"model/\"+target_formula+\".model\"):\n",
    "    mixed_curriculum_train(rnn, train_set, stop_threshold=stop_threshold)\n",
    "    rnn.save_model()\n",
    "else:\n",
    "    print(\"loading already saved model\")\n",
    "    rnn.load_model()\n",
    "    \n",
    "rnn.renew()\n",
    "dfa_from_rnn = rnn\n",
    "# statistics\n",
    "\n",
    "def percent(num, digits=2):\n",
    "    tens = pow(10, digits)\n",
    "    return int(100*num*tens)/tens\n",
    "\n",
    "print(\"testing on train set, i.e. test set is train set\")\n",
    "# we're printing stats on the train set for now, but you can define other test sets by using\n",
    "# make_train_set_for_target\n",
    "\n",
    "pos = 0\n",
    "rnn_target = 0\n",
    "for w in test_set:\n",
    "    if generator_dfa.classify_word(w):\n",
    "        pos += 1\n",
    "\n",
    "    if dfa_from_rnn.classify_word(w) == generator_dfa.classify_word(w):\n",
    "        rnn_target += 1\n",
    "test_acc = percent(rnn_target/test_set_size)\n",
    "print(\"rnn score against target on test set:                             \",\n",
    "        rnn_target, \"(\"+str(test_acc)+\")\")\n",
    "        \n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F(|(x230,x3))\nlearned LTL formula:false\n0  iteration complete\n\n\n\nlearned LTL formula:true\n1  iteration complete\n\n\n\nlearned LTL formula:x774\n2  iteration complete\n\n\n\nlearned LTL formula:(F x230)\n\n\nepsilon= 0.03 delta= 0.03 max_trace_length= 20\nquery: F(|(x230,x3))\nfinal ltl:  (F x230)\nreturned counterexamples: ['x774x15x230', '', 'x774']\ntime learner: 4.594047784805298\ntime verifier: 12.810323238372803\nRandom words: 541\nTime taken: 28.076335906982422\n\nExplanation matches RNN: 95.83\nRNN matches ground truth: 97.72\nExplanation matches ground truth: 96.03\n"
    }
   ],
   "source": [
    "from samples2ltl.utils.SimpleTree import Formula\n",
    "\n",
    "timeout = 30\n",
    "maximum_sequence_length = 500\n",
    "maximum_formula_depth = 50\n",
    "epsilon = 0.05\n",
    "delta = 0.05\n",
    "\n",
    "\n",
    "# use a query LTL formula\n",
    "text_formula = 'F(|(x230,x3))'\n",
    "query_formula = Formula.convertTextToFormula(text_formula)\n",
    "print(query_formula)\n",
    "\n",
    "# print(query_formula)\n",
    "# query_dfa=ltlf2dfa.translate_ltl2dfa(alphabet=[\"X_\" + str(character) for character in alphabet],formula=query_formula, token=\"bal\")\n",
    "# print(query_dfa)\n",
    "\"\"\"  \n",
    "Create initial samples\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from lexr.RNNexplainer import Traces\n",
    "traces=Traces(rnn, alphabet, token=\"bal\")\n",
    "traces.label_from_network([])\n",
    "traces.write_in_file()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PACTeacher.pac_teacher import PACTeacher as Teacher \n",
    "explainer=Explainer(alphabet=[character for character in alphabet], token=\"bal\")\n",
    "teacher = Teacher(dfa_from_rnn,epsilon=.03, delta=.03, max_trace_length=20, max_formula_depth=10, query_dfa=query_formula)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "from multiprocessing import Process, Queue\n",
    "explainer, flag, learner_time, verification_time = teacher.teach(explainer, traces, timeout = timeout, verbose = False)\n",
    "end_time=time.time()\n",
    "\n",
    "\n",
    "print(\"\\n\\nepsilon=\", teacher.epsilon, \"delta=\", teacher.delta, \"max_trace_length=\", teacher.max_trace_length)\n",
    "print(\"query:\", text_formula)\n",
    "print(\"final ltl: \", explainer.ltl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_delta = None\n",
    "new_epsilon = None\n",
    "if(not flag):\n",
    "    print(\"incomplete formula\")\n",
    "    new_delta, new_epsilon = teacher.calculate_revised_delta_and_epsilon()\n",
    "    print(new_delta, new_epsilon)\n",
    "\n",
    "\n",
    "print(\"returned counterexamples:\", teacher.returned_counterexamples)\n",
    "\n",
    "print(\"time learner:\", learner_time)\n",
    "print('time verifier:', verification_time)\n",
    "print(\"Random words:\", teacher.number_of_words_checked)\n",
    "print(\"Time taken:\", end_time-start_time)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "run_lstar = False\n",
    "if(run_lstar):\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        # compare with dfa from lstar_algorithm\n",
    "        dfa_from_rnn.renew()\n",
    "        start_time_lstar = time.time()\n",
    "        dfa_lstar, lstar_flag = extract(rnn, query=query_formula, max_trace_length=maximum_sequence_length, epsilon=delta,\n",
    "                                        delta=delta, time_limit=timeout, initial_split_depth=10, starting_examples=[])\n",
    "        end_time_lstar = time.time()\n",
    "\n",
    "        dfa_lstar = dfa_lstar.minimize_()\n",
    "        dfa_lstar.draw_nicely(\n",
    "            filename=target_formula+\":\"+text_formula+\"_\"+str(epsilon)+\"_\"+str(delta))\n",
    "\n",
    "        \n",
    "        print(\"\\nTime taken to extract lstar-dfa:\",\n",
    "                end_time_lstar-start_time_lstar)\n",
    "        print(\"returned flag:\", lstar_flag)\n",
    "        print(\"transitions:->\")\n",
    "        # print(dfa_lstar.delta)\n",
    "        num_lstar_states = len(dfa_lstar.Q)\n",
    "        print(\"number of states of the dfa:\", num_lstar_states)\n",
    "\n",
    "\n",
    "performance_explanation_with_rnn = performance_rnn_with_groundtruth = performance_explanation_with_groundtruth = 0\n",
    "lstar_performance_explanation_with_rnn = lstar_performance_explanation_with_groundtruth = 0\n",
    "\n",
    "test_set_size = 0\n",
    "for w in test_set:\n",
    "\n",
    "    \n",
    "\n",
    "    dfa_from_rnn.renew()\n",
    "\n",
    "    test_set_size += 1\n",
    "    verdict_rnn = dfa_from_rnn.classify_word(w)\n",
    "    verdict_target = generator_dfa.classify_word(w)\n",
    "    trace_vector = []\n",
    "    if(\"x\" in w):\n",
    "        for letter in w.split(\"x\")[1:]:\n",
    "            if(target_formula == \"Text classification\" and letter == '0'):\n",
    "                continue\n",
    "            trace_vector.append([alphabet[i] == \"x\" + letter for i in range(len(alphabet))])\n",
    "    else: \n",
    "        for letter in w:\n",
    "            trace_vector.append([alphabet[i] == letter for i in range(len(alphabet))])\n",
    "    if(len(w) == 0):\n",
    "        trace = Trace([[False for _ in alphabet]])\n",
    "    else:\n",
    "        trace = Trace(trace_vector)\n",
    "\n",
    "    verdict_ltl = trace.evaluateFormulaOnTrace(explainer.formula)\n",
    "    verdict_query =  trace.evaluateFormulaOnTrace(query_formula)\n",
    "\n",
    "    if(run_lstar):\n",
    "        verdict_lstar = dfa_lstar.classify_word(w)\n",
    "\n",
    "    if (verdict_rnn and verdict_query) == verdict_ltl:\n",
    "        performance_explanation_with_rnn += 1\n",
    "    if verdict_rnn == verdict_target:\n",
    "        performance_rnn_with_groundtruth += 1\n",
    "    if verdict_ltl == (verdict_target and verdict_query):\n",
    "        performance_explanation_with_groundtruth += 1\n",
    "    if(run_lstar):\n",
    "        if (verdict_rnn and verdict_query) == verdict_lstar:\n",
    "            lstar_performance_explanation_with_rnn += 1\n",
    "        # else:\n",
    "        #     print(w, verdict_lstar, verdict_rnn, verdict_query)\n",
    "        if verdict_lstar == (verdict_target and verdict_query):\n",
    "            lstar_performance_explanation_with_groundtruth += 1\n",
    "        # else:\n",
    "        #     print(w, verdict_lstar, verdict_target, verdict_query)\n",
    "\n",
    "\n",
    "if(test_set_size != 0):\n",
    "    print(\"Explanation matches RNN:\", str(\n",
    "        percent(performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "    print(\"RNN matches ground truth:\", str(\n",
    "        percent(performance_rnn_with_groundtruth/test_set_size)))\n",
    "\n",
    "    print(\"Explanation matches ground truth:\", str(\n",
    "        percent(performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "    if(run_lstar):\n",
    "        print(\"Lstar matches RNN:\", str(\n",
    "            percent(lstar_performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "        print(\"Lstar matches ground truth:\", str(\n",
    "            percent(lstar_performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "\n",
    "\n",
    "if(not run_lstar):\n",
    "    num_lstar_states = None\n",
    "    start_time_lstar = 0\n",
    "    end_time_lstar = 0\n",
    "    lstar_performance_explanation_with_rnn = 0\n",
    "    lstar_performance_explanation_with_groundtruth = 0\n",
    "    lstar_flag = False\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['X(x0)', 'F(x0)', '!(x1)', '!(x2)', 'x0', 'x2']\n"
    }
   ],
   "source": [
    "f = open(\"benchmarks/query/\" + target_formula + \".txt\")\n",
    "query_formulas = f.readline()[1:-1].split(\", \")\n",
    "query_formulas = list(set([query_formula[1:-1] for query_formula in query_formulas]))\n",
    "f.close()\n",
    "print(query_formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}