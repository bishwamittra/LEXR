{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from RNN2DFA.LSTM import LSTMNetwork\n",
    "# from GRU import GRUNetwork\n",
    "from RNN2DFA.RNNClassifier import RNNClassifier\n",
    "from RNN2DFA.Training_Functions import mixed_curriculum_train\n",
    "from RNN2DFA.Training_Functions import make_test_set,make_train_set_for_target\n",
    "from RNNexplainer import Explainer\n",
    "import pandas as pd\n",
    "import LTL2DFA as ltlf2dfa\n",
    "from RNN2DFA.Extraction import extract\n",
    "import pickle\n",
    "from samples2ltl.utils.Traces import Trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reber grammar\n",
    "from specific_examples import Reber_Grammar\n",
    "rg=Reber_Grammar()\n",
    "alphabet=rg.alphabet\n",
    "generator_dfa=rg\n",
    "sample_train_set=[]\n",
    "for i in range(100):\n",
    "    seq, _, _ = rg.get_one_example(maxLength=10)\n",
    "    sample_train_set.append(seq)\n",
    "    # print(rg.classify_word(rg.sequenceToWord(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F(a)\n"
    }
   ],
   "source": [
    "import specific_examples\n",
    "generator_dfa=specific_examples.Example4()\n",
    "target_formula = generator_dfa.target_formula\n",
    "alphabet = generator_dfa.alphabet\n",
    "query_formulas = generator_dfa.query_formulas\n",
    "print(target_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loading from previously stored benchmarks\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def dict2lists(dictionary):\n",
    "    X,y=[],[]\n",
    "    for key in dictionary:\n",
    "        X.append(key)\n",
    "        y.append(dictionary[key])\n",
    "    return X,y\n",
    "\n",
    "def lists2dict(x,y):\n",
    "    # both x and y should have same length\n",
    "    assert len(x)==len(y), \"Error dimension\"\n",
    "    d={}\n",
    "    n=len(x)\n",
    "    for idx in range(n):\n",
    "        d[x[idx]]=y[idx]\n",
    "    return d\n",
    "\n",
    "num_layers = 2\n",
    "num_hidden_dim = 10\n",
    "input_dim = 3\n",
    "iterations = 1\n",
    "stop_threshold = 0.0005\n",
    "RNNClass = LSTMNetwork\n",
    "if(target_formula == \"DNA sequence\"):\n",
    "    num_layers = 5\n",
    "    num_hidden_dim = 10\n",
    "    input_dim = 5\n",
    "    stop_threshold = 0.1\n",
    "    RNNClass = LSTMNetwork\n",
    "\n",
    "if(target_formula == \"Text classification\"):\n",
    "    num_layers = 2\n",
    "    num_hidden_dim = 10\n",
    "    input_dim = 5\n",
    "    stop_threshold = 0.06\n",
    "    RNNClass = LSTMNetwork\n",
    "\n",
    "\n",
    "# for each example, specify a different generating function\n",
    "file_name = \"benchmarks/\" + target_formula.replace(\" \", \"_\")+\".pkl\"\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "\n",
    "    if(target_formula == \"balanced parentheses\"):\n",
    "\n",
    "        train_set = generator_dfa.get_balanced_parantheses_train_set(8000, 2, 50, max_train_samples_per_length=3000,\n",
    "                                                                     search_size_per_length=2000, lengths=[i for i in range(maximum_sequence_length+1)])\n",
    "\n",
    "    elif(target_formula == \"email match\"):\n",
    "\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=1000,\n",
    "                                              search_size_per_length=3000, deviation=200)\n",
    "\n",
    "        # generate more examples that match the regular expression\n",
    "        matching_strings = generator_dfa.generate_matching_strings(\n",
    "            n=10800, max_length=50)\n",
    "        for string in matching_strings:\n",
    "            train_set[string] = True\n",
    "\n",
    "    elif(target_formula == \"alternating bit protocol\"):\n",
    "\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=1000,\n",
    "                                              search_size_per_length=3000, deviation=250)\n",
    "\n",
    "        # generate more examples that match the regular expression\n",
    "        matching_strings = generator_dfa.generate_matching_strings(\n",
    "            n=105000, max_sequence_length=50)\n",
    "        for string in matching_strings:\n",
    "            train_set[string] = True\n",
    "    elif(target_formula == 'G(a->X(b))'):\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=1000,\n",
    "                                              search_size_per_length=3000, deviation=20)\n",
    "    elif(target_formula == \"DNA sequence\" or target_formula == \"Text classification\"):\n",
    "        train_set = generator_dfa.dict\n",
    "        \n",
    "    else:\n",
    "        train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet, lengths=[i for i in range(maximum_sequence_length+1)],\n",
    "                                              max_train_samples_per_length=10000,\n",
    "                                              search_size_per_length=30000, deviation=20)\n",
    "\n",
    "    # now save the dataset to file\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        pickle.dump(train_set, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else:\n",
    "    # load the dataset\n",
    "    print(\"loading from previously stored benchmarks\")\n",
    "\n",
    "    def load_obj(name):\n",
    "        with open(name, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    train_set = load_obj(file_name)\n",
    "\n",
    "\n",
    "   \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F(a)\nout of  6088  sequences 3519  are positive. (percent:  0.5780223390275953 )\nexamples per length: [1, 3, 9, 27, 52, 84, 148, 276, 532, 1044, 1068, 710, 502, 316, 234, 150, 92, 78, 58, 42, 30, 30, 26, 24, 24, 22, 24, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\nEmpty string was already included in train set\nEmpty string status: False\nsize of train set: 4870\nsize of test set: 1218\nconfigurations: layers:  2 hidden dimension:  10 input dim:  3 network:  <class 'RNN2DFA.LSTM.LSTMNetwork'> stop threshold:  0.0005\nThe dy.parameter(...) call is now DEPRECATED.\n        There is no longer need to explicitly add parameters to the computation graph.\n        Any used parameter will be added automatically.\nloading already saved model\ntesting on train set, i.e. test set is train set\nrnn score against target on test set:                              1218 (100.0)\n"
    }
   ],
   "source": [
    "print(target_formula)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print ratio\n",
    "cnt = 0\n",
    "examples_per_length = [0 for i in range(51)]\n",
    "for key in train_set:\n",
    "    if(train_set[key]):\n",
    "        cnt += 1\n",
    "    try:\n",
    "        examples_per_length[len(key)] += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "total_samples = len(train_set)\n",
    "print(\"out of \", total_samples, \" sequences\", cnt,\n",
    "      \" are positive. (percent: \", float(cnt/total_samples), \")\")\n",
    "print(\"examples per length:\", examples_per_length)\n",
    "\n",
    "# split train:test\n",
    "X, y = dict2lists(train_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "train_set = lists2dict(X_train, y_train)\n",
    "test_set = lists2dict(X_test, y_test)\n",
    "train_set_size = len(train_set)\n",
    "test_set_size = len(test_set)\n",
    "\n",
    "# intentionally pushing \"\" (empty string) in train_set\n",
    "if('' not in train_set):\n",
    "    try:\n",
    "        train_set[''] = test_set['']\n",
    "        print(\"Empty string status:\", train_set[''])\n",
    "    except:\n",
    "        print(\"Empty string is not in test test also\")\n",
    "else:\n",
    "    print(\"Empty string was already included in train set\")\n",
    "    print(\"Empty string status:\", train_set[''])\n",
    "\n",
    "print(\"size of train set:\", train_set_size)\n",
    "print(\"size of test set:\", test_set_size)\n",
    "\n",
    "print(\"configurations: layers: \", num_layers,\n",
    "          \"hidden dimension: \", num_hidden_dim,\n",
    "          \"input dim: \", input_dim,\n",
    "          \"network: \", RNNClass,\n",
    "          \"stop threshold: \", stop_threshold)\n",
    "\n",
    "\n",
    "# define rnn\n",
    "rnn = RNNClassifier(alphabet, num_layers=num_layers,\n",
    "                    hidden_dim=num_hidden_dim, RNNClass=RNNClass, input_dim=input_dim, target=target_formula)\n",
    "\n",
    "try:\n",
    "    # train the model\n",
    "    if not os.path.isfile(\"model/\"+target_formula+\".model\"):\n",
    "        mixed_curriculum_train(rnn, train_set, stop_threshold=stop_threshold)\n",
    "        rnn.save_model()\n",
    "    else:\n",
    "        print(\"loading already saved model\")\n",
    "        rnn.load_model()\n",
    "except:\n",
    "    print(\"Training error: however moving on as life also goes on\" )\n",
    "    \n",
    "rnn.renew()\n",
    "dfa_from_rnn = rnn\n",
    "# statistics\n",
    "\n",
    "def percent(num, digits=2):\n",
    "    tens = pow(10, digits)\n",
    "    return int(100*num*tens)/tens\n",
    "\n",
    "print(\"testing on train set, i.e. test set is train set\")\n",
    "# we're printing stats on the train set for now, but you can define other test sets by using\n",
    "# make_train_set_for_target\n",
    "\n",
    "pos = 0\n",
    "rnn_target = 0\n",
    "for w in test_set:\n",
    "    if generator_dfa.classify_word(w):\n",
    "        pos += 1\n",
    "\n",
    "    if dfa_from_rnn.classify_word(w) == generator_dfa.classify_word(w):\n",
    "        rnn_target += 1\n",
    "test_acc = percent(rnn_target/test_set_size)\n",
    "print(\"rnn score against target on test set:                             \",\n",
    "        rnn_target, \"(\"+str(test_acc)+\")\")\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F(x0)\n\n\npositive traces---> \n[]\n\n\nnegative traces---> \n[]\n\n\n\nLearning formula with depth0\nNumber of subformulas:1\nlearned LTL formula:false\nLearning took:  0.050098419189453125  s\nEQ test took  0.03376483917236328  s\nnew counterexample: a  should be accepted by implementation\n\n\npositive traces---> \n['a']\n\n\nnegative traces---> \n[]\n\n\n\n0  iteration complete\n\n\n\nLearning formula with depth0\nNumber of subformulas:1\nlearned LTL formula:true\nLearning took:  0.047107696533203125  s\nEQ test took  0.0007648468017578125  s\nnew counterexample:   should be rejected by implementation\n\n\npositive traces---> \n['a']\n\n\nnegative traces---> \n['']\n\n\n\n1  iteration complete\n\n\n\nstart formula depth:1\nBefore normalization:x0\nLearning formula with depth0\nNumber of subformulas:1\nlearned LTL formula:a\nLearning took:  0.07786822319030762  s\nNo negative counterexample found\nEQ test took  0.08665060997009277  s\nnew counterexample: ca  should be accepted by implementation\n\n\npositive traces---> \n['a', 'ca']\n\n\nnegative traces---> \n['']\n\n\n\n2  iteration complete\n\n\n\nstart formula depth:1\nincreasing formula depth to2\nBefore normalization:(F x0)\nLearning formula with depth1\nNumber of subformulas:2\nlearned LTL formula:(F a)\nLearning took:  0.10121488571166992  s\nEQ test took  0.17935466766357422  s\n\n\nepsilon= 0.03 delta= 0.03 max_trace_length= 20\nquery: F(x0)\nfinal ltl:  (F a)\nreturned counterexamples: ['a', '', 'ca']\ntime learner: 0.2762892246246338\ntime verifier: 0.30053067207336426\nRandom words: 541\n\nTime taken: 0.5843605995178223\n\n\n\n\n\ndoing minimization from  2  states to  2  states\noverall guided extraction time took: 0.05986166000366211\ngenerated counterexamples were: (format: (counterexample, counterexample generation time))\n\ndoing minimization from  2  states to  2  states\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTime taken to extract lstar-dfa: 0.060451507568359375\nreturned flag: True\ntransitions:->\nnumber of states of the dfa: 2\nExplanation matches RNN: 100.0\nRNN matches ground truth: 100.0\nExplanation matches ground truth: 100.0\nLstar matches RNN: 100.0\nLstar matches ground truth: 100.0\n"
    }
   ],
   "source": [
    "from samples2ltl.utils.SimpleTree import Formula\n",
    "\n",
    "timeout = 10\n",
    "maximum_sequence_length = 50\n",
    "maximum_formula_depth = 50\n",
    "epsilon = 0.05\n",
    "delta = 0.05\n",
    "\n",
    "\n",
    "# use a query LTL formula\n",
    "text_formula = \"F(x0)\"\n",
    "query_formula = Formula.convertTextToFormula(text_formula)\n",
    "print(query_formula)\n",
    "\n",
    "# print(query_formula)\n",
    "# query_dfa=ltlf2dfa.translate_ltl2dfa(alphabet=[\"X_\" + str(character) for character in alphabet],formula=query_formula, token=\"bal\")\n",
    "# print(query_dfa)\n",
    "\"\"\"  \n",
    "Create initial samples\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from RNNexplainer import Traces\n",
    "traces=Traces(rnn, alphabet, token=\"bal\")\n",
    "traces.label_from_network([])\n",
    "traces.write_in_file()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PACTeacher.pac_teacher import PACTeacher as Teacher \n",
    "explainer=Explainer(alphabet=[character for character in alphabet], token=\"bal\")\n",
    "teacher = Teacher(dfa_from_rnn,epsilon=.03, delta=.03, max_trace_length=20, max_formula_depth=10, query_dfa=query_formula)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "from multiprocessing import Process, Queue\n",
    "explainer, flag, learner_time, verification_time = teacher.teach(explainer, traces, timeout = timeout)\n",
    "end_time=time.time()\n",
    "\n",
    "\n",
    "print(\"\\n\\nepsilon=\", teacher.epsilon, \"delta=\", teacher.delta, \"max_trace_length=\", teacher.max_trace_length)\n",
    "print(\"query:\", query_formula)\n",
    "print(\"final ltl: \", explainer.ltl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_delta = None\n",
    "new_epsilon = None\n",
    "if(not flag):\n",
    "    print(\"incomplete formula\")\n",
    "    new_delta, new_epsilon = teacher.calculate_revised_delta_and_epsilon()\n",
    "    print(new_delta, new_epsilon)\n",
    "\n",
    "\n",
    "print(\"returned counterexamples:\", teacher.returned_counterexamples)\n",
    "\n",
    "print(\"time learner:\", learner_time)\n",
    "print('time verifier:', verification_time)\n",
    "print(\"Random words:\", teacher.number_of_words_checked)\n",
    "print(\"\\nTime taken:\", end_time-start_time)\n",
    "\n",
    "\n",
    "\n",
    "run_lstar = True\n",
    "if(run_lstar):\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        # compare with dfa from lstar_algorithm\n",
    "        dfa_from_rnn.renew()\n",
    "        start_time_lstar = time.time()\n",
    "        dfa_lstar, lstar_flag = extract(rnn, query=query_formula, max_trace_length=maximum_sequence_length, epsilon=delta,\n",
    "                                        delta=delta, time_limit=timeout, initial_split_depth=10, starting_examples=[])\n",
    "        end_time_lstar = time.time()\n",
    "\n",
    "        dfa_lstar = dfa_lstar.minimize_()\n",
    "        dfa_lstar.draw_nicely(\n",
    "            filename=target_formula+\":\"+text_formula+\"_\"+str(epsilon)+\"_\"+str(delta))\n",
    "\n",
    "        \n",
    "        print(\"\\nTime taken to extract lstar-dfa:\",\n",
    "                end_time_lstar-start_time_lstar)\n",
    "        print(\"returned flag:\", lstar_flag)\n",
    "        print(\"transitions:->\")\n",
    "        # print(dfa_lstar.delta)\n",
    "        num_lstar_states = len(dfa_lstar.Q)\n",
    "        print(\"number of states of the dfa:\", num_lstar_states)\n",
    "\n",
    "\n",
    "performance_explanation_with_rnn = performance_rnn_with_groundtruth = performance_explanation_with_groundtruth = 0\n",
    "lstar_performance_explanation_with_rnn = lstar_performance_explanation_with_groundtruth = 0\n",
    "\n",
    "test_set_size = 0\n",
    "for w in train_set:\n",
    "\n",
    "    \n",
    "\n",
    "    dfa_from_rnn.renew()\n",
    "\n",
    "    test_set_size += 1\n",
    "    verdict_rnn = dfa_from_rnn.classify_word(w)\n",
    "    verdict_target = generator_dfa.classify_word(w)\n",
    "    trace_vector = []\n",
    "    for letter in w:\n",
    "        trace_vector.append([alphabet[i] == letter for i in range(len(alphabet))])\n",
    "    \n",
    "    if(len(w) == 0):\n",
    "        trace = Trace([[False for _ in alphabet]])\n",
    "    else:\n",
    "        trace = Trace(trace_vector)\n",
    "\n",
    "    verdict_ltl = trace.evaluateFormulaOnTrace(explainer.formula)\n",
    "    verdict_query =  trace.evaluateFormulaOnTrace(query_formula)\n",
    "\n",
    "    if(run_lstar):\n",
    "        verdict_lstar = dfa_lstar.classify_word(w)\n",
    "\n",
    "    if (verdict_rnn and verdict_query) == verdict_ltl:\n",
    "        performance_explanation_with_rnn += 1\n",
    "    if verdict_rnn == verdict_target:\n",
    "        performance_rnn_with_groundtruth += 1\n",
    "    if verdict_ltl == (verdict_target and verdict_query):\n",
    "        performance_explanation_with_groundtruth += 1\n",
    "    if(run_lstar):\n",
    "        if (verdict_rnn and verdict_query) == verdict_lstar:\n",
    "            lstar_performance_explanation_with_rnn += 1\n",
    "        if verdict_lstar == (verdict_target and verdict_query):\n",
    "            lstar_performance_explanation_with_groundtruth += 1\n",
    "\n",
    "if(test_set_size != 0):\n",
    "    print(\"Explanation matches RNN:\", str(\n",
    "        percent(performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "    print(\"RNN matches ground truth:\", str(\n",
    "        percent(performance_rnn_with_groundtruth/test_set_size)))\n",
    "\n",
    "    print(\"Explanation matches ground truth:\", str(\n",
    "        percent(performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "    if(run_lstar):\n",
    "        print(\"Lstar matches RNN:\", str(\n",
    "            percent(lstar_performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "        print(\"Lstar matches ground truth:\", str(\n",
    "            percent(lstar_performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "\n",
    "\n",
    "if(not run_lstar):\n",
    "    num_lstar_states = None\n",
    "    start_time_lstar = 0\n",
    "    end_time_lstar = 0\n",
    "    lstar_performance_explanation_with_rnn = 0\n",
    "    lstar_performance_explanation_with_groundtruth = 0\n",
    "    lstar_flag = False\n",
    "\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}