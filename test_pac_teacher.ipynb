{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from RNN2DFA.LSTM import LSTMNetwork\n",
    "# from GRU import GRUNetwork\n",
    "from RNN2DFA.RNNClassifier import RNNClassifier\n",
    "from RNN2DFA.Training_Functions import mixed_curriculum_train\n",
    "import Tomita_Grammars \n",
    "from RNN2DFA.Training_Functions import make_test_set,make_train_set_for_target\n",
    "from RNNexplainer import Explainer\n",
    "import pandas as pd\n",
    "import LTL2DFA as ltlf2dfa\n",
    "from RNN2DFA.Extraction import extract\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reber grammar\n",
    "from specific_examples import Reber_Grammar\n",
    "rg=Reber_Grammar()\n",
    "alphabet=rg.alphabet\n",
    "generator_dfa=rg\n",
    "sample_train_set=[]\n",
    "for i in range(100):\n",
    "    seq, _, _ = rg.get_one_example(maxLength=10)\n",
    "    sample_train_set.append(seq)\n",
    "    # print(rg.classify_word(rg.sequenceToWord(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import specific_examples\n",
    "generator_dfa=specific_examples.Example4()\n",
    "target_formula = generator_dfa.target_formula\n",
    "alphabet = generator_dfa.alphabet\n",
    "query_formulas = generator_dfa.query_formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "made train set of size: 1229 , of which positive examples: 1089\nout of  1229  sequences 1089  are positive. (percent:  0.8860862489829129 )\nexamples per length: [0, 3, 9, 27, 52, 82, 66, 50, 40, 46, 38, 30, 22, 22, 20, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\nsize of train set: 983\nsize of test set: 246\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def dict2lists(dictionary):\n",
    "    X,y=[],[]\n",
    "    for key in dictionary:\n",
    "        X.append(key)\n",
    "        y.append(dictionary[key])\n",
    "    return X,y\n",
    "\n",
    "def lists2dict(x,y):\n",
    "    # both x and y should have same length\n",
    "    assert len(x)==len(y), \"Error dimension\"\n",
    "    d={}\n",
    "    n=len(x)\n",
    "    for idx in range(n):\n",
    "        d[x[idx]]=y[idx]\n",
    "    return d\n",
    "\n",
    "# for each example, specify a different generating function\n",
    "\n",
    "\n",
    "if(target_formula == \"balanced parentheses\"):\n",
    "    train_set = generator_dfa.get_balanced_parantheses_train_set(100, 2, 50,max_train_samples_per_length=300,                                                         search_size_per_length=20,lengths=[i+1 for i in range(50)])\n",
    "    \n",
    "\n",
    "elif(target_formula == \"email match\"):\n",
    "    train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet,lengths=[i+1 for i in range(50)],\n",
    "                                          max_train_samples_per_length=1000,\n",
    "                                          search_size_per_length=3000, deviation = 20)\n",
    "\n",
    "    # generate more examples that match the regular expression\n",
    "    matching_strings = generator_dfa.generate_matching_strings(\n",
    "        n=1080, max_length=50)\n",
    "    for string in matching_strings:\n",
    "        train_set[string] = True\n",
    "    \n",
    "    \n",
    "\n",
    "elif(target_formula == \"alternating bit protocol\"):\n",
    "    train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet,lengths=[i+1 for i in range(50)],\n",
    "                                          max_train_samples_per_length=100,\n",
    "                                          search_size_per_length=30, deviation = 25)\n",
    "\n",
    "    # generate more examples that match the regular expression\n",
    "    matching_strings = generator_dfa.generate_matching_strings(\n",
    "        n=1050, max_sequence_length=50)\n",
    "    for string in matching_strings:\n",
    "        train_set[string] = True\n",
    "\n",
    "else:\n",
    "    train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet,lengths=[i+1 for i in range(50)],\n",
    "                                          max_train_samples_per_length=100,\n",
    "                                          search_size_per_length=300, deviation = 20)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# print ratio\n",
    "cnt = 0\n",
    "examples_per_length=[0 for i in range(51)]\n",
    "for key in train_set:\n",
    "    if(train_set[key]):\n",
    "        cnt += 1\n",
    "    examples_per_length[len(key)] += 1\n",
    "\n",
    "total_samples = len(train_set) \n",
    "print(\"out of \", total_samples, \" sequences\", cnt , \" are positive. (percent: \", float(cnt/total_samples), \")\")\n",
    "print(\"examples per length:\", examples_per_length)\n",
    "    \n",
    "# split train:test\n",
    "X, y = dict2lists(train_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "train_set = lists2dict(X_train, y_train)\n",
    "test_set = lists2dict(X_test, y_test)\n",
    "print(\"size of train set:\", len(train_set))\n",
    "print(\"size of test set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F(a)\nThe dy.parameter(...) call is now DEPRECATED.\n        There is no longer need to explicitly add parameters to the computation graph.\n        Any used parameter will be added automatically.\ncurrent average loss is:  0.004664791964770603\ncurrent average loss is:  0.0007406270763992021\nclassification loss on last batch was: 0.0004842058968303416\ntesting on train set, i.e. test set is train set\ntest set size: 246\nrnn score against target on test set:                              246 (100.0)\n"
    }
   ],
   "source": [
    "fout=open(\"output/log.txt\", \"a\")\n",
    "fout.write(\".........................................................................\\n\")\n",
    "fout.write(\"Target: \"+ target_formula)\n",
    "print(target_formula)\n",
    "fout.write(\"\\n\")\n",
    "fout.close()\n",
    "\n",
    "\n",
    "# define rnn\n",
    "rnn = RNNClassifier(alphabet,num_layers=1,hidden_dim=10,RNNClass = LSTMNetwork)\n",
    "\n",
    "\n",
    "# train the model\n",
    "mixed_curriculum_train(rnn,train_set,stop_threshold = 0.0005)\n",
    "rnn.renew()  \n",
    "dfa_from_rnn=rnn \n",
    "# statistics\n",
    "\n",
    "def percent(num,digits=2):\n",
    "    tens = pow(10,digits)\n",
    "    return int(100*num*tens)/tens\n",
    "\n",
    "print(\"testing on train set, i.e. test set is train set\")\n",
    "# we're printing stats on the train set for now, but you can define other test sets by using\n",
    "# make_train_set_for_target\n",
    "\n",
    "n = len(test_set)\n",
    "print(\"test set size:\", n)\n",
    "pos = 0\n",
    "rnn_target = 0\n",
    "for w in test_set:\n",
    "    if generator_dfa.classify_word(w):\n",
    "        pos+=1\n",
    "\n",
    "    if dfa_from_rnn.classify_word(w)==generator_dfa.classify_word(w):\n",
    "        rnn_target+=1\n",
    "print(\"rnn score against target on test set:                             \",rnn_target,\"(\"+str(percent(rnn_target/n))+\")\")\n",
    "test_set_size= len(test_set)\n",
    "test_acc = percent(rnn_target/test_set_size)\n",
    "\n",
    "# dfa_from_rnn=generator_dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "false\n\n\npositive traces---> \n[]\n\n\nnegative traces---> \n[]\n\n\n\nLearning formula with depth 0\nlearned LTL formula: true\nLearning took:  0.06396198272705078  s\nEQ test took  0.004388093948364258  s\nnew counterexample:   should be rejected by implementation\n\n\npositive traces---> \n[]\n\n\nnegative traces---> \n['']\n\n\n\n0  iteration complete\n\n\n\nLearning formula with depth 0\nlearned LTL formula: false\nLearning took:  0.06078910827636719  s\nEQ test took  0.1442410945892334  s\n\n\nepsilon= 0.03 delta= 0.03 max_trace_length= 20\nquery: false\nfinal ltl:  false\nreturned counterexamples: ['']\n\nTime taken: 0.2774050235748291\ntarget  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size\n  F(a)  false       false   True          100.0      None              None                              None         0.277405          None            None              []        983       246\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# use a query LTL formula\n",
    "query_formula=\"false\"\n",
    "print(query_formula)\n",
    "query_dfa=ltlf2dfa.translate_ltl2dfa(alphabet=[character for character in alphabet],formula=query_formula, token=\"bal\")\n",
    "# print(query_dfa)\n",
    "\"\"\"  \n",
    "Create initial samples\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from RNNexplainer import Traces\n",
    "traces=Traces(rnn, alphabet, token=\"bal\")\n",
    "traces.label_from_network([])\n",
    "traces.write_in_file()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PACTeacher.pac_teacher import PACTeacher as Teacher \n",
    "explainer=Explainer(alphabet=[character for character in alphabet], token=\"bal\")\n",
    "teacher = Teacher(dfa_from_rnn,epsilon=.03, delta=.03, max_trace_length=20, max_formula_depth=10, query_dfa=query_dfa)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "from multiprocessing import Process, Queue\n",
    "explainer, flag= teacher.teach(explainer, traces, timeout = 15)\n",
    "end_time=time.time()\n",
    "\n",
    "\n",
    "print(\"\\n\\nepsilon=\", teacher.epsilon, \"delta=\", teacher.delta, \"max_trace_length=\", teacher.max_trace_length)\n",
    "print(\"query:\", query_formula)\n",
    "print(\"final ltl: \", explainer.ltl)\n",
    "\n",
    "fout=open(\"output/log.txt\", \"a\")\n",
    "fout.write(\"\\n\\nquery: \"+query_formula)\n",
    "fout.write(\"\\nfinal LTL: \"+ explainer.ltl)\n",
    "\n",
    "new_delta = None\n",
    "new_epsilon = None\n",
    "if(not flag):\n",
    "    fout.write(\" [incomplete]\")\n",
    "    print(\"incomplete formula\")\n",
    "    new_delta, new_epsilon = teacher.calculate_revised_delta_and_epsilon()\n",
    "    print(new_delta, new_epsilon)\n",
    "\n",
    "fout.write(\"\\n\\n\")\n",
    "\n",
    "print(\"returned counterexamples:\", teacher.returned_counterexamples)\n",
    "\n",
    "print(\"\\nTime taken:\", end_time-start_time)\n",
    "fout.close()\n",
    "\n",
    "\n",
    "fout=open(\"output/log.txt\", \"a\")\n",
    "# fout.write(\"rnn score against target on test set:                             \"+str(rnn_target)+\"(\"+str(percent(rnn_target/n))+\")\")\n",
    "# fout.write(\"\\n\")\n",
    "\n",
    "performance_explanation_with_rnn = performance_rnn_with_groundtruth = performance_explanation_with_groundtruth = 0\n",
    "\n",
    "test_set_size = 0\n",
    "for w in train_set:\n",
    "    if(query_dfa.classify_word(w)):\n",
    "        test_set_size += 1\n",
    "        verdict_rnn = dfa_from_rnn.classify_word(w)\n",
    "        verdict_target = generator_dfa.classify_word(w)\n",
    "        verdict_ltl = explainer.dfa.classify_word(w)\n",
    "        \n",
    "        \n",
    "        if verdict_rnn == verdict_ltl:\n",
    "            performance_explanation_with_rnn += 1\n",
    "        if verdict_rnn == verdict_target:\n",
    "            performance_rnn_with_groundtruth +=1\n",
    "        if verdict_ltl == verdict_target:\n",
    "            performance_explanation_with_groundtruth +=1\n",
    "\n",
    "if(test_set_size != 0):\n",
    "\n",
    "\n",
    "    print(\"Explanation matches RNN:\", str(percent(performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "    print(\"RNN matches ground truth:\", str(percent(performance_rnn_with_groundtruth/test_set_size)))\n",
    "\n",
    "    print(\"Explanation matches ground truth:\", str(percent(performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "\n",
    "fout.close()\n",
    "\n",
    "\n",
    "# report in a pandas file\n",
    "result = pd.DataFrame(columns=['target', \n",
    "                                'query', \n",
    "                                'explanation', \n",
    "                                'status',\n",
    "                                'test accuracy', \n",
    "                                'rnn score', \n",
    "                                'explanation score', \n",
    "                                'explanation score on ground truth',\n",
    "                                'extraction time',\n",
    "                                'revised delta',\n",
    "                                'revised epsilon',\n",
    "                                'counterexamples',\n",
    "                                'train size',\n",
    "                                'test size'\n",
    "                                ])\n",
    "\n",
    "if(test_set_size != 0):\n",
    "    result = result.append(\n",
    "        {\n",
    "            'target':target_formula,\n",
    "            'query':query_formula,\n",
    "            'explanation':explainer.ltl,\n",
    "            'status':flag,\n",
    "            'test accuracy':test_acc,\n",
    "            'rnn score': percent(performance_rnn_with_groundtruth/test_set_size),\n",
    "            'explanation score':percent(performance_explanation_with_rnn/test_set_size),\n",
    "            'explanation score on ground truth':percent(performance_explanation_with_groundtruth/test_set_size),\n",
    "            'extraction time': end_time-start_time, \n",
    "            'revised delta': new_delta, \n",
    "            'revised epsilon': new_epsilon, \n",
    "            'counterexamples': teacher.returned_counterexamples,\n",
    "            'train size': len(train_set),\n",
    "            'test size': len(test_set)\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "else:\n",
    "    result = result.append(\n",
    "        {\n",
    "            'target':target_formula,\n",
    "            'query':query_formula,\n",
    "            'explanation':explainer.ltl,\n",
    "            'status':flag,\n",
    "            'test accuracy':test_acc,\n",
    "            'rnn score': None,\n",
    "            'explanation score':None,\n",
    "            'explanation score on ground truth':None,\n",
    "            'extraction time': end_time-start_time, \n",
    "            'revised delta': new_delta, \n",
    "            'revised epsilon': new_epsilon, \n",
    "            'counterexamples': teacher.returned_counterexamples,\n",
    "            'train size': len(train_set),\n",
    "            'test size': len(test_set)\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "print(result.to_string(index=False))\n",
    "result.to_csv('output/result.csv', header=False, index=False, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "provided counterexamples are: []\n\n\nReturned counterexample is:   which should be classified:  False\nbad counterexample - already saved and classified in table!\n\n\nNo positive counterexample found\nReturned counterexample is: a  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: aa  which should be classified:  False\nbad counterexample - already saved and classified in table!\n\n\nNo positive counterexample found\nReturned counterexample is: ac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: abb  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: baa  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: caa  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: abc  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: aba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cbab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ccba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cbba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bbba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bcba  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ccac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bcab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ccab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bcaa  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bcac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cbac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bcca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cbaa  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ccca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cbca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ccaa  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bbca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bbbac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bcbca  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bbcaa  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: ccbac  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: bbbab  which should be classified:  False\n\n\nNo positive counterexample found\nReturned counterexample is: cbcab  which should be classified:  False\nobs table timed out\nTotal time taken: 5.134732\noverall guided extraction time took: 5.135629999999999\ngenerated counterexamples were: (format: (counterexample, counterexample generation time))\n('', 1.4999999997655777e-05)\n('a', 0.023386000000002127)\n('ab', 0.025392000000003634)\n('aa', 0.028556000000001802)\n('ac', 0.03279099999999602)\n('ca', 0.037267999999997414)\n('ba', 0.04067299999999818)\n('cca', 0.04451999999999856)\n('bca', 0.0538749999999979)\n('cac', 0.053909999999994795)\n('bba', 0.05139499999999941)\n('cab', 0.05541600000000102)\n('abb', 0.08470599999999706)\n('baa', 0.07449100000000186)\n('caa', 0.06926599999999894)\n('cba', 0.09591299999999592)\n('abc', 0.08652299999999968)\n('aba', 0.09510500000000377)\n('bab', 0.08616599999999863)\n('bac', 0.09662200000000354)\n('cbab', 0.11189599999999444)\n('ccba', 0.09746599999999717)\n('cbba', 0.10127500000000111)\n('bbba', 0.10770899999999983)\n('bcba', 0.10939900000000335)\n('ccac', 0.11694500000000119)\n('bcab', 0.13418500000000222)\n('ccab', 0.11815699999999651)\n('bcaa', 0.12463300000000288)\n('bcac', 0.12373700000000554)\n('cbac', 0.13700100000000504)\n('bcca', 0.15319999999999823)\n('cbaa', 0.14398300000000575)\n('ccca', 0.14909000000000106)\n('cbca', 0.15071900000000227)\n('ccaa', 0.15295299999999656)\n('bbca', 0.16484599999999716)\n('bbbac', 0.146967999999994)\n('bcbca', 0.16617899999999963)\n('bbcaa', 0.19626699999999886)\n('ccbac', 0.1723479999999995)\n('bbbab', 0.1749409999999969)\n('cbcab', 0.1833079999999967)\nnumber of states of the dfa: 58\nreturned flag: False\n"
    }
   ],
   "source": [
    "# all_words = sorted(list(train_set.keys()),key=lambda x:len(x))\n",
    "# pos = next((w for w in all_words if rnn.classify_word(w) and query_dfa.classify_word(w) ==True),None)\n",
    "# neg = next((w for w in all_words if rnn.classify_word(w) and query_dfa.classify_word(w) ==False),None)\n",
    "# starting_examples = [w for w in [pos,neg] if not None == w]\n",
    "# print(starting_examples)\n",
    "starting_examples = []\n",
    "dfa_lstar, flag = extract(rnn, query=query_dfa, max_trace_length = 20, epsilon=0.03, delta= 0.03, time_limit = 5,initial_split_depth = 10,starting_examples=starting_examples)\n",
    "dfa_lstar.draw_nicely(maximum=50)\n",
    "print(\"number of states of the dfa:\", len(dfa_lstar.Q) )\n",
    "print(\"returned flag:\", flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}