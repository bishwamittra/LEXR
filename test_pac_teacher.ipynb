{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from RNN2DFA.LSTM import LSTMNetwork\n",
    "# from GRU import GRUNetwork\n",
    "from RNN2DFA.RNNClassifier import RNNClassifier\n",
    "from RNN2DFA.Training_Functions import mixed_curriculum_train\n",
    "import Tomita_Grammars \n",
    "from RNN2DFA.Training_Functions import make_test_set,make_train_set_for_target\n",
    "from RNNexplainer import Explainer\n",
    "import pandas as pd\n",
    "import LTL2DFA as ltlf2dfa\n",
    "from RNN2DFA.Extraction import extract\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reber grammar\n",
    "from specific_examples import Reber_Grammar\n",
    "rg=Reber_Grammar()\n",
    "alphabet=rg.alphabet\n",
    "generator_dfa=rg\n",
    "sample_train_set=[]\n",
    "for i in range(100):\n",
    "    seq, _, _ = rg.get_one_example(maxLength=10)\n",
    "    sample_train_set.append(seq)\n",
    "    # print(rg.classify_word(rg.sequenceToWord(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import specific_examples\n",
    "generator_dfa=specific_examples.Example4()\n",
    "target_formula = generator_dfa.target_formula\n",
    "alphabet = generator_dfa.alphabet\n",
    "query_formulas = generator_dfa.query_formulas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "made train set of size: 1215 , of which positive examples: 1082\nout of  1215  sequences 1082  are positive. (percent:  0.8905349794238683 )\nexamples per length: [0, 3, 9, 27, 52, 82, 54, 50, 46, 42, 38, 22, 24, 24, 22, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\nsize of train set: 972\nsize of test set: 243\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def dict2lists(dictionary):\n",
    "    X,y=[],[]\n",
    "    for key in dictionary:\n",
    "        X.append(key)\n",
    "        y.append(dictionary[key])\n",
    "    return X,y\n",
    "\n",
    "def lists2dict(x,y):\n",
    "    # both x and y should have same length\n",
    "    assert len(x)==len(y), \"Error dimension\"\n",
    "    d={}\n",
    "    n=len(x)\n",
    "    for idx in range(n):\n",
    "        d[x[idx]]=y[idx]\n",
    "    return d\n",
    "\n",
    "# for each example, specify a different generating function\n",
    "\n",
    "\n",
    "if(target_formula == \"balanced parentheses\"):\n",
    "    train_set = generator_dfa.get_balanced_parantheses_train_set(100, 2, 50,max_train_samples_per_length=300,                                                         search_size_per_length=20,lengths=[i+1 for i in range(50)])\n",
    "    \n",
    "\n",
    "elif(target_formula == \"email match\"):\n",
    "    train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet,lengths=[i+1 for i in range(50)],\n",
    "                                          max_train_samples_per_length=1000,\n",
    "                                          search_size_per_length=3000, deviation = 20)\n",
    "\n",
    "    # generate more examples that match the regular expression\n",
    "    matching_strings = generator_dfa.generate_matching_strings(\n",
    "        n=1080, max_length=50)\n",
    "    for string in matching_strings:\n",
    "        train_set[string] = True\n",
    "    \n",
    "    \n",
    "\n",
    "elif(target_formula == \"alternating bit protocol\"):\n",
    "    train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet,lengths=[i+1 for i in range(50)],\n",
    "                                          max_train_samples_per_length=100,\n",
    "                                          search_size_per_length=30, deviation = 25)\n",
    "\n",
    "    # generate more examples that match the regular expression\n",
    "    matching_strings = generator_dfa.generate_matching_strings(\n",
    "        n=1050, max_sequence_length=50)\n",
    "    for string in matching_strings:\n",
    "        train_set[string] = True\n",
    "\n",
    "else:\n",
    "    train_set = make_train_set_for_target(generator_dfa.classify_word, alphabet,lengths=[i+1 for i in range(50)],\n",
    "                                          max_train_samples_per_length=100,\n",
    "                                          search_size_per_length=300, deviation = 20)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# print ratio\n",
    "cnt = 0\n",
    "examples_per_length=[0 for i in range(51)]\n",
    "for key in train_set:\n",
    "    if(train_set[key]):\n",
    "        cnt += 1\n",
    "    examples_per_length[len(key)] += 1\n",
    "\n",
    "total_samples = len(train_set) \n",
    "print(\"out of \", total_samples, \" sequences\", cnt , \" are positive. (percent: \", float(cnt/total_samples), \")\")\n",
    "print(\"examples per length:\", examples_per_length)\n",
    "    \n",
    "# split train:test\n",
    "X, y = dict2lists(train_set)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "train_set = lists2dict(X_train, y_train)\n",
    "test_set = lists2dict(X_test, y_test)\n",
    "print(\"size of train set:\", len(train_set))\n",
    "print(\"size of test set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "F(a)\nThe dy.parameter(...) call is now DEPRECATED.\n        There is no longer need to explicitly add parameters to the computation graph.\n        Any used parameter will be added automatically.\ncurrent average loss is:  0.007794793926774946\ncurrent average loss is:  0.0014236712070391038\nclassification loss on last batch was: 0.000492587968149558\ntesting on train set, i.e. test set is train set\ntest set size: 243\nrnn score against target on test set:                              243 (100.0)\n"
    }
   ],
   "source": [
    "fout=open(\"output/log.txt\", \"a\")\n",
    "fout.write(\".........................................................................\\n\")\n",
    "fout.write(\"Target: \"+ target_formula)\n",
    "print(target_formula)\n",
    "fout.write(\"\\n\")\n",
    "fout.close()\n",
    "\n",
    "\n",
    "# define rnn\n",
    "rnn = RNNClassifier(alphabet,num_layers=1,hidden_dim=10,RNNClass = LSTMNetwork)\n",
    "\n",
    "\n",
    "# train the model\n",
    "mixed_curriculum_train(rnn,train_set,stop_threshold = 0.0005)\n",
    "rnn.renew()  \n",
    "dfa_from_rnn=rnn \n",
    "# statistics\n",
    "\n",
    "def percent(num,digits=2):\n",
    "    tens = pow(10,digits)\n",
    "    return int(100*num*tens)/tens\n",
    "\n",
    "print(\"testing on train set, i.e. test set is train set\")\n",
    "# we're printing stats on the train set for now, but you can define other test sets by using\n",
    "# make_train_set_for_target\n",
    "\n",
    "n = len(test_set)\n",
    "print(\"test set size:\", n)\n",
    "pos = 0\n",
    "rnn_target = 0\n",
    "for w in test_set:\n",
    "    if generator_dfa.classify_word(w):\n",
    "        pos+=1\n",
    "\n",
    "    if dfa_from_rnn.classify_word(w)==generator_dfa.classify_word(w):\n",
    "        rnn_target+=1\n",
    "print(\"rnn score against target on test set:                             \",rnn_target,\"(\"+str(percent(rnn_target/n))+\")\")\n",
    "test_set_size= len(test_set)\n",
    "test_acc = percent(rnn_target/test_set_size)\n",
    "\n",
    "# dfa_from_rnn=generator_dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 83)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m83\u001b[0m\n\u001b[0;31m    performance_explanation_with_rnn = performance_rnn_with_groundtruth = performance_explanation_with_groundtruth = 0\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "timeout = 15\n",
    "maximum_sequence_length = 50\n",
    "maximum_formula_depth = 50\n",
    "epsilon = 0.05\n",
    "delta = 0.05\n",
    "\n",
    "# use a query LTL formula\n",
    "query_formula=\"true\"\n",
    "print(query_formula)\n",
    "query_dfa=ltlf2dfa.translate_ltl2dfa(alphabet=[character for character in alphabet],formula=query_formula, token=\"bal\")\n",
    "# print(query_dfa)\n",
    "\"\"\"  \n",
    "Create initial samples\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from RNNexplainer import Traces\n",
    "traces=Traces(rnn, alphabet, token=\"bal\")\n",
    "traces.label_from_network([])\n",
    "traces.write_in_file()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PACTeacher.pac_teacher import PACTeacher as Teacher \n",
    "explainer=Explainer(alphabet=[character for character in alphabet], token=\"bal\")\n",
    "teacher = Teacher(dfa_from_rnn,epsilon=.03, delta=.03, max_trace_length=20, max_formula_depth=10, query_dfa=query_dfa)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "from multiprocessing import Process, Queue\n",
    "explainer, flag= teacher.teach(explainer, traces, timeout = 2)\n",
    "end_time=time.time()\n",
    "\n",
    "\n",
    "print(\"\\n\\nepsilon=\", teacher.epsilon, \"delta=\", teacher.delta, \"max_trace_length=\", teacher.max_trace_length)\n",
    "print(\"query:\", query_formula)\n",
    "print(\"final ltl: \", explainer.ltl)\n",
    "\n",
    "fout=open(\"output/log.txt\", \"a\")\n",
    "fout.write(\"\\n\\nquery: \"+query_formula)\n",
    "fout.write(\"\\nfinal LTL: \"+ explainer.ltl)\n",
    "\n",
    "new_delta = None\n",
    "new_epsilon = None\n",
    "if(not flag):\n",
    "    fout.write(\" [incomplete]\")\n",
    "    print(\"incomplete formula\")\n",
    "    new_delta, new_epsilon = teacher.calculate_revised_delta_and_epsilon()\n",
    "    print(new_delta, new_epsilon)\n",
    "\n",
    "fout.write(\"\\n\\n\")\n",
    "\n",
    "print(\"returned counterexamples:\", teacher.returned_counterexamples)\n",
    "\n",
    "print(\"\\nTime taken:\", end_time-start_time)\n",
    "fout.close()\n",
    "\n",
    "\n",
    "fout=open(\"output/log.txt\", \"a\")\n",
    "\n",
    "run_lstar = False\n",
    "if(run_lstar):\n",
    "        # compare with dfa from lstar_algorithm\n",
    "        dfa_from_rnn.renew()\n",
    "        start_time_lstar = time.time()\n",
    "        dfa_lstar, lstar_flag = extract(rnn, query=query_dfa, max_trace_length=maximum_sequence_length, epsilon=delta,\n",
    "                                        delta=delta, time_limit=timeout, initial_split_depth=10, starting_examples=[])\n",
    "        end_time_lstar = time.time()\n",
    "\n",
    "        dfa_lstar.draw_nicely(\n",
    "            filename=target_formula+\":\"+query_formula+\"_\"+str(epsilon)+\"_\"+str(delta))\n",
    "        print(\"\\nTime taken to extract lstar-dfa:\",\n",
    "                end_time_lstar-start_time_lstar)\n",
    "        print(\"number of states of the dfa:\", len(dfa_lstar.Q))\n",
    "        print(\"returned flag:\", lstar_flag)\n",
    "        print(\"transitions:->\")\n",
    "        print(dfa_lstar.delta)\n",
    "        num_lstar_states = len(dfa_lstar.Q)\n",
    "\n",
    "performance_explanation_with_rnn = performance_rnn_with_groundtruth = performance_explanation_with_groundtruth = 0\n",
    "lstar_performance_explanation_with_rnn = lstar_performance_explanation_with_groundtruth = 0\n",
    "\n",
    "test_set_size = 0\n",
    "for w in saved_sequences:\n",
    "\n",
    "    if(query_dfa.classify_word(w)):\n",
    "        dfa_from_rnn.renew()\n",
    "        test_set_size += 1\n",
    "        verdict_rnn = dfa_from_rnn.classify_word(w)\n",
    "        verdict_target = generator_dfa.classify_word(w)\n",
    "        verdict_ltl = explainer.dfa.classify_word(w)\n",
    "        if(run_lstar):\n",
    "            verdict_lstar = dfa_lstar.classify_word(w)\n",
    "\n",
    "        if verdict_rnn == verdict_ltl:\n",
    "            performance_explanation_with_rnn += 1\n",
    "        if verdict_rnn == verdict_target:\n",
    "            performance_rnn_with_groundtruth += 1\n",
    "        if verdict_ltl == verdict_target:\n",
    "            performance_explanation_with_groundtruth += 1\n",
    "        if(run_lstar):\n",
    "            if verdict_rnn == verdict_lstar:\n",
    "                lstar_performance_explanation_with_rnn += 1\n",
    "            if verdict_lstar == verdict_target:\n",
    "                lstar_performance_explanation_with_groundtruth += 1\n",
    "\n",
    "if(test_set_size != 0):\n",
    "    print(\"Explanation matches RNN:\", str(\n",
    "        percent(performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "    print(\"RNN matches ground truth:\", str(\n",
    "        percent(performance_rnn_with_groundtruth/test_set_size)))\n",
    "\n",
    "    print(\"Explanation matches ground truth:\", str(\n",
    "        percent(performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "    if(run_lstar):\n",
    "        print(\"Lstar matches RNN:\", str(\n",
    "            percent(lstar_performance_explanation_with_rnn/test_set_size)))\n",
    "\n",
    "        print(\"Lstar matches ground truth:\", str(\n",
    "            percent(lstar_performance_explanation_with_groundtruth/test_set_size)))\n",
    "\n",
    "fout.close()\n",
    "\n",
    "if(not run_lstar):\n",
    "    num_lstar_states = None\n",
    "    start_time_lstar = 0\n",
    "    end_time_lstar = 0\n",
    "    lstar_performance_explanation_with_rnn = 0\n",
    "    lstar_performance_explanation_with_groundtruth = 0\n",
    "    lstar_flag = False\n",
    "\n",
    "\n",
    "# report in a pandas file\n",
    "result = pd.DataFrame(columns=['target',\n",
    "                                'query',\n",
    "                                'explanation',\n",
    "                                'status',\n",
    "                                'test accuracy',\n",
    "                                'rnn score',\n",
    "                                'explanation score',\n",
    "                                'explanation score on ground truth',\n",
    "                                'extraction time',\n",
    "                                'revised delta',\n",
    "                                'revised epsilon',\n",
    "                                'counterexamples',\n",
    "                                'train size',\n",
    "                                'test size',\n",
    "                                'ltl_depth',\n",
    "                                'lstar_states',\n",
    "                                'lstar explanation score',\n",
    "                                'lstar explanation score on ground truth',\n",
    "                                'lstar extraction time',\n",
    "                                'lstar_status',\n",
    "                                'epsilon',\n",
    "                                'delta'\n",
    "                                ])\n",
    "\n",
    "if(test_set_size != 0):\n",
    "    result = result.append(\n",
    "        {\n",
    "            'target': target_formula,\n",
    "            'query': query_formula,\n",
    "            'explanation': explainer.ltl,\n",
    "            'status': flag,\n",
    "            'test accuracy': test_acc,\n",
    "            'rnn score': percent(performance_rnn_with_groundtruth/test_set_size),\n",
    "            'explanation score': percent(performance_explanation_with_rnn/test_set_size),\n",
    "            'explanation score on ground truth': percent(performance_explanation_with_groundtruth/test_set_size),\n",
    "            'extraction time': end_time-start_time,\n",
    "            'revised delta': new_delta,\n",
    "            'revised epsilon': new_epsilon,\n",
    "            'counterexamples': teacher.returned_counterexamples,\n",
    "            'train size': len(train_set),\n",
    "            'test size': len(test_set),\n",
    "            \"ltl_depth\": explainer.formula_depth,\n",
    "            \"lstar_states\":num_lstar_states,\n",
    "            'lstar explanation score': percent(lstar_performance_explanation_with_rnn/test_set_size),\n",
    "            'lstar explanation score on ground truth': percent(lstar_performance_explanation_with_groundtruth/test_set_size),\n",
    "            'lstar extraction time': end_time_lstar - start_time_lstar,\n",
    "            'lstar_status': lstar_flag,\n",
    "            'epsilon': epsilon,\n",
    "            'delta': delta\n",
    "\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "else:\n",
    "    result = result.append(\n",
    "        {\n",
    "            'target': target_formula,\n",
    "            'query': query_formula,\n",
    "            'explanation': explainer.ltl,\n",
    "            'status': flag,\n",
    "            'test accuracy': test_acc,\n",
    "            'rnn score': None,\n",
    "            'explanation score': None,\n",
    "            'explanation score on ground truth': None,\n",
    "            'extraction time': end_time-start_time,\n",
    "            'revised delta': new_delta,\n",
    "            'revised epsilon': new_epsilon,\n",
    "            'counterexamples': teacher.returned_counterexamples,\n",
    "            'train size': len(train_set),\n",
    "            'test size': len(test_set),\n",
    "            'train size': len(train_set),\n",
    "            'test size': len(test_set),\n",
    "            \"ltl_depth\": explainer.formula_depth,\n",
    "            \"lstar_states\": num_lstar_states,\n",
    "            'lstar explanation score': None,\n",
    "            'lstar explanation score on ground truth': None,\n",
    "            'lstar extraction time': end_time_lstar - start_time_lstar,\n",
    "            'lstar_status': lstar_flag,\n",
    "            'epsilon': epsilon,\n",
    "            'delta': delta\n",
    "\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "print(result.to_string(index=False))\n",
    "result.to_csv('output/result.csv', header=False,\n",
    "                index=False, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}