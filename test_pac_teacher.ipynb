{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "from RNN2DFA.LSTM import LSTMNetwork\n",
    "# from GRU import GRUNetwork\n",
    "from RNN2DFA.RNNClassifier import RNNClassifier\n",
    "from RNN2DFA.Training_Functions import mixed_curriculum_train\n",
    "import Tomita_Grammars \n",
    "from lstar_extraction.Training_Functions import make_test_set,make_train_set_for_target\n",
    "from RNNexplainer import Explainer\n",
    "import pandas as pd\n",
    "import LTL2DFA as ltlf2dfa\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dfa that implements alternating bit protocol\n",
    "import LTL2DFA as ltlf2dfa\n",
    "dfa=ltlf2dfa.DFA()\n",
    "\n",
    "# alphabet\n",
    "alphabet=\"abcd\" \n",
    "\"\"\" \n",
    "a = bit 0 \n",
    "b = bit 1\n",
    "c = bit 0 acknowledge\n",
    "d = bit 1 acknowledge\n",
    " \"\"\"\n",
    "dfa.alphabet = [character for character in alphabet]\n",
    "dfa.Q = [0,1,2,3,4] # state 4 denotes rejecting state for all unalowed move\n",
    "dfa.q0 = 0\n",
    "dfa.F= [0,2] \n",
    "dfa.delta = {\n",
    "    0: {\n",
    "        '0001': 0,\n",
    "        '0010': 4,\n",
    "        '0100': 4,\n",
    "        '1000': 1,\n",
    "        '0000': 0 # implements empty input\n",
    "    },\n",
    "    1: {\n",
    "        '0001': 4,\n",
    "        '0010': 2,\n",
    "        '0100': 4,\n",
    "        '1000': 1,\n",
    "        '0000': 1 # implements empty input\n",
    "    },\n",
    "    2: {\n",
    "        '0001': 4,\n",
    "        '0010': 2,\n",
    "        '0100': 3,\n",
    "        '1000': 4,\n",
    "        '0000': 2 # implements empty input\n",
    "    },\n",
    "    3: {\n",
    "        '0001': 0,\n",
    "        '0010': 4,\n",
    "        '0100': 3,\n",
    "        '1000': 4,\n",
    "        '0000': 3 # implements empty input\n",
    "    },\n",
    "    4: {\n",
    "        '0001': 4,\n",
    "        '0010': 4,\n",
    "        '0100': 4,\n",
    "        '1000': 4,\n",
    "        '0000': 4 # implements empty input\n",
    "    }\n",
    "}\n",
    "target_formula=\"alternating bit protocol\"\n",
    "generator_dfa=dfa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "made train set of size: 366 , of which positive examples: 100\n\n\nPositive examples:\npaqd\nqapd\nqaqd\npapd\nppapd\npqaqd\nqnaqd\nqqaqd\npnaqd\nqaqpd\nqapqd\npmaqd\nqappd\nqmaqd\nqqapd\nqmapd\nqaqqd\nqnapd\npappd\npnapd\npmapd\npapqd\npaqqd\nqpaqd\nqpapd\nppaqd\npaqpd\npqapd\nqappqd\nqmmapd\nqpmapd\npaqpqd\nqnnaqd\npqnaqd\npmaqpd\nqpnapd\npmqapd\nqmnaqd\npqpapd\npmaqqd\nqnpapd\nqppaqd\npmpaqd\nqqpaqd\nqqnapd\npnappd\npappqd\npapppd\npnnapd\nqqmaqd\nppqaqd\npnaqpd\npnqapd\npppapd\nqnapqd\nqapqqd\npmappd\npnapqd\nqnpaqd\npmpapd\nppqapd\nqqapqd\nqmaqqpd\nqqmapqd\nqppaqpd\npqqpapd\npmpnapd\nqmqnaqd\nqppmaqd\npmqapqd\nqqaqpqd\npmnnapd\nqnppapd\npmpapqd\nqpnaqpd\npnpqaqd\npnqappd\npappqpd\nqpqpaqd\nqnapqqd\npaqpqqd\npqappqd\nqmmapqd\npqqqapd\nqppmapd\nqqnaqpd\nppmnqapd\nqqpnapqd\nqpqpaqpd\npnmnaqqd\nqpqmnaqd\npmmpqnaqd\nppmnaqqpd\nqmmmpnaqd\npnqnnmaqd\npqnnapppd\nqpnqnmaqd\nppppqnappd\npnmnqmqqaqpd\npqqnmmqapqqd\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# make training set on grammars\n",
    "target = Tomita_Grammars.tomita_email\n",
    "alphabet = \"pqradmn\"\n",
    "generator_dfa.classify_word=target\n",
    "target_formula=\"mail string match\"\n",
    "\n",
    "\n",
    "# # use a dfa to generate training set\n",
    "# target_formula=\"G(b->X(~a))\"\n",
    "# alphabet=\"abc\"\n",
    "# fout=open(\"output/log.txt\", \"a\")\n",
    "# fout.write(\".........................................................................\\n\")\n",
    "# fout.write(\"Target: \"+ target_formula)\n",
    "# fout.write(\"\\n\")\n",
    "# fout.close()\n",
    "# import LTL2DFA as ltlf2dfa\n",
    "# generator_dfa=ltlf2dfa.translate_ltl2dfa(alphabet=[character for character in alphabet],formula=target_formula)\n",
    "# print(generator_dfa)\n",
    "\n",
    "\n",
    "train_set = make_train_set_for_target(generator_dfa.classify_word,alphabet)\n",
    "\n",
    "# define rnn\n",
    "rnn = RNNClassifier(alphabet,num_layers=1,hidden_dim=10,RNNClass = LSTMNetwork)\n",
    "\n",
    "\n",
    "# print(train_set)\n",
    "print(\"\\n\\nPositive examples:\")\n",
    "for key in train_set:\n",
    "    if(train_set[key]):\n",
    "        print(key)\n",
    "# print(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "current average loss is:  0.1753828246865356\ncurrent average loss is:  0.07014514845714234\ncurrent average loss is:  0.04538774555712416\ncurrent average loss is:  0.050422424940686474\ncurrent average loss is:  0.020722455132687306\ncurrent average loss is:  0.01311825012248722\ncurrent average loss is:  0.009999133198639083\ncurrent average loss is:  0.007804854971463563\ncurrent average loss is:  0.0063430318415490655\ncurrent average loss is:  0.005328063430681906\ncurrent average loss is:  0.0026285493744357506\ncurrent average loss is:  0.0015394853385116314\ncurrent average loss is:  0.0009797159053401595\ncurrent average loss is:  0.000657816575492912\nclassification loss on last batch was: 0.0004897981487661845\ntesting on train set, i.e. test set is train set\ntest set size: 366\nof which positive: 100 (27.32)\nrnn score against target on test set:                              266 (72.67)\n"
    }
   ],
   "source": [
    "# train the model\n",
    "mixed_curriculum_train(rnn,train_set,stop_threshold = 0.0005)\n",
    "rnn.renew()  \n",
    "dfa_from_rnn=rnn \n",
    "# statistics\n",
    "\n",
    "def percent(num,digits=2):\n",
    "    tens = pow(10,digits)\n",
    "    return int(100*num*tens)/tens\n",
    "\n",
    "test_set = train_set \n",
    "print(\"testing on train set, i.e. test set is train set\")\n",
    "# we're printing stats on the train set for now, but you can define other test sets by using\n",
    "# make_train_set_for_target\n",
    "\n",
    "n = len(test_set)\n",
    "print(\"test set size:\", n)\n",
    "pos = len([w for w in test_set if generator_dfa.classify_word(w)])\n",
    "print(\"of which positive:\",pos,\"(\"+str(percent(pos/n))+\")\")\n",
    "rnn_target = len([w for w in test_set if dfa_from_rnn.classify_word(w)==generator_dfa.classify_word(w)])\n",
    "print(\"rnn score against target on test set:                             \",rnn_target,\"(\"+str(percent(rnn_target/n))+\")\")\n",
    "\n",
    "# dfa_from_rnn=generator_dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "made train set of size: 15654 , of which positive examples: 7874\n\n\npositive traces---> \n['rapd']\n\n\nnegative traces---> \n['a']\n\n\n\nstart formula depth: 1\nlearned LTL formula: r\nnew counterexample: maanqapamqnqd  should be accepted by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a']\n\n\n\n0  iteration complete\n\n\n\nstart formula depth: 1\nlearned LTL formula: (F p)\nnew counterexample: p  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p']\n\n\n\n1  iteration complete\n\n\n\nstart formula depth: 2\nlearned LTL formula: (X a)\nnew counterexample: ma  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma']\n\n\n\n2  iteration complete\n\n\n\nstart formula depth: 2\nlearned LTL formula: (F d)\nnew counterexample: nmqmramnrpqd  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd']\n\n\n\n3  iteration complete\n\n\n\nstart formula depth: 2\nlearned LTL formula: (X (X (~ q)))\nnew counterexample: mdm  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd', 'mdm']\n\n\n\n4  iteration complete\n\n\n\nstart formula depth: 4\nlearned LTL formula: ((F a) U (X p))\nnew counterexample: mranp  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd', 'mdm', 'mranp']\n\n\n\n5  iteration complete\n\n\n\nstart formula depth: 5\nlearned LTL formula: (X ((F d) & a))\nnew counterexample: qapnmrpmdr  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd', 'mdm', 'mranp', 'qapnmrpmdr']\n\n\n\n6  iteration complete\n\n\n\nstart formula depth: 5\nlearned LTL formula: ((X (r -> d)) U (G d))\nnew counterexample: nqnqd  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd', 'mdm', 'mranp', 'qapnmrpmdr', 'nqnqd']\n\n\n\n7  iteration complete\n\n\n\nstart formula depth: 6\nlearned LTL formula: ((F (a & (X a))) | r)\nnew counterexample: r  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd', 'mdm', 'mranp', 'qapnmrpmdr', 'nqnqd', 'r']\n\n\n\n8  iteration complete\n\n\n\nstart formula depth: 6\nlearned LTL formula: ((X a) & (F (G d)))\nnew counterexample: aaaanmqdqd  should be rejected by implementation\n\n\npositive traces---> \n['rapd', 'maanqapamqnqd']\n\n\nnegative traces---> \n['a', 'p', 'ma', 'nmqmramnrpqd', 'mdm', 'mranp', 'qapnmrpmdr', 'nqnqd', 'r', 'aaaanmqdqd']\n\n\n\n9  iteration complete\n\n\n\n\n\nepsilon= 0.05 delta= 0.05 max_trace_length= 20\nquery: F(a)\nfinal ltl:  ((X a) & (F (G d)))\nincomplete formula\n\nTime taken: 52.129985000000005\nextracted LTL score against rnn on test set:                       173 (90.1)\nextracted LTL score against target on rnn's test set:              173 (90.1)\nextracted LTL score against rnn on test set (with query):          173 (90.1)\nextracted LTL score against target on rnn's test set (with query): 173 (90.1)\n            target query          explanation status  rnn score  explanation score  explanation score on ground truth  extraction time\n mail string match  F(a)  ((X a) & (F (G d)))  False      93.22               90.1                               90.1        52.129985\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# use a query LTL formula\n",
    "query_formula=\"F(a)\"\n",
    "query_dfa=ltlf2dfa.translate_ltl2dfa(alphabet=[character for character in alphabet],formula=query_formula)\n",
    "\n",
    "\n",
    "\n",
    "# make test_set supported by query dfa\n",
    "sample_test_set=make_train_set_for_target(query_dfa.classify_word, alphabet)\n",
    "\n",
    "# only consider sample that are true\n",
    "test_set=[]\n",
    "for key in sample_test_set:\n",
    "    if(sample_test_set[key]):\n",
    "        test_set.append(key)\n",
    "# print(test_set[:30])\n",
    "\n",
    "from PACTeacher.pac_teacher import PACTeacher as Teacher \n",
    "\n",
    "# query_dfa=None\n",
    "\n",
    "if(query_dfa is None):\n",
    "    query_formula=None\n",
    "    test_set=make_test_set(alphabet)\n",
    "    raise SystemError\n",
    "\n",
    "\n",
    "from RNNexplainer import Traces\n",
    "traces=Traces(rnn, alphabet)\n",
    "traces.label_from_network(test_set)\n",
    "traces.write_in_file()\n",
    "\n",
    "\n",
    "explainer=Explainer(alphabet=[character for character in alphabet])\n",
    "teacher = Teacher(dfa_from_rnn,epsilon=.05, delta=.05, max_trace_length=20, max_formula_depth=10, query_dfa=query_dfa)\n",
    "from time import clock\n",
    "start_time=clock()\n",
    "flag=teacher.teach(explainer,traces)\n",
    "end_time=clock()\n",
    "\n",
    "\n",
    "print(\"\\n\\nepsilon=\", teacher.epsilon, \"delta=\", teacher.delta, \"max_trace_length=\", teacher.max_trace_length)\n",
    "print(\"query:\", query_formula)\n",
    "print(\"final ltl: \", explainer.ltl)\n",
    "\n",
    "fout=open(\"output/log.txt\", \"a\")\n",
    "fout.write(\"\\n\\nquery: \"+query_formula)\n",
    "fout.write(\"\\nfinal LTL: \"+ explainer.ltl)\n",
    "if(not flag):\n",
    "    fout.write(\" [incomplete]\")\n",
    "    print(\"incomplete formula\")\n",
    "fout.write(\"\\n\\n\")\n",
    "\n",
    "print(\"\\nTime taken:\", end_time-start_time)\n",
    "\n",
    "fout.close()\n",
    "\n",
    "\n",
    "test_set = train_set \n",
    "\n",
    "fout=open(\"output/log.txt\", \"a\")\n",
    "# fout.write(\"rnn score against target on test set:                             \"+str(rnn_target)+\"(\"+str(percent(rnn_target/n))+\")\")\n",
    "# fout.write(\"\\n\")\n",
    "\n",
    "performance_ltl = len([w for w in test_set if dfa_from_rnn.classify_word(w)==explainer.dfa.classify_word(w)])\n",
    "print(\"extracted LTL score against rnn on test set:                      \",performance_ltl,\"(\"+str(percent(performance_ltl/n))+\")\")\n",
    "performance_ltl_with_target = len([w for w in test_set if explainer.dfa.classify_word(w)==generator_dfa.classify_word(w)])\n",
    "print(\"extracted LTL score against target on rnn's test set:             \",performance_ltl_with_target,\"(\"+str(percent(performance_ltl_with_target/n))+\")\")\n",
    "\n",
    "performance_ltl = len([w for w in test_set if (dfa_from_rnn.classify_word(w)and query_dfa.classify_word(w)) ==explainer.dfa.classify_word(w)])\n",
    "print(\"extracted LTL score against rnn on test set (with query):         \",performance_ltl,\"(\"+str(percent(performance_ltl/n))+\")\")\n",
    "# fout.write(\"extracted LTL score against rnn on test set (with query):         \"+str(performance_ltl)+\"(\"+str(percent(performance_ltl/n))+\")\\n\")\n",
    "performance_ltl_with_target = len([w for w in test_set if explainer.dfa.classify_word(w)== (generator_dfa.classify_word(w) and query_dfa.classify_word(w))])\n",
    "print(\"extracted LTL score against target on rnn's test set (with query):\",performance_ltl_with_target,\"(\"+str(percent(performance_ltl_with_target/n))+\")\")\n",
    "\n",
    "# fout.write(\"extracted LTL score against target on rnn's test set (with query):\"+str(performance_ltl_with_target)+\"(\"+str(percent(performance_ltl_with_target/n))+\")\\n\")\n",
    "fout.close()\n",
    "\n",
    "\n",
    "# report in a pandas file\n",
    "result = pd.DataFrame(columns=['target', \n",
    "                                'query', \n",
    "                                'explanation', \n",
    "                                'status', \n",
    "                                'rnn score', \n",
    "                                'explanation score', \n",
    "                                'explanation score on ground truth',\n",
    "                                'extraction time'\n",
    "                                ])\n",
    "\n",
    "result = result.append(\n",
    "    {\n",
    "        'target':target_formula,\n",
    "        'query':query_formula,\n",
    "        'explanation':explainer.ltl,\n",
    "        'status':flag,\n",
    "        'rnn score':percent(rnn_target/n),\n",
    "        'explanation score':percent(performance_ltl/n),\n",
    "        'explanation score on ground truth':percent(performance_ltl_with_target/n),\n",
    "        'extraction time': end_time-start_time\n",
    "    }, ignore_index=True\n",
    ")\n",
    "print(result.to_string(index=False))\n",
    "result.to_csv('output/result.csv', header=False, index=False, mode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "target                      query                      explanation  status  rnn score  explanation score  explanation score on ground truth  extraction time\n alternating bit protocol  a&X(c)&X(X(b))&X(X(X(d)))                            false    True     100.00              99.76                              99.76         0.096940\n alternating bit protocol                    a->F(c)      (~ (d U ((b | c) | (G a))))   False     100.00              73.98                              73.98       217.447062\n alternating bit protocol  (a->F(c))&(G(~b))&(G(~d))   (((d | a) | (c | b)) -> (X c))   False     100.00              73.26                              73.26        71.138986\n        mail string match                       G(p)                            false    True       0.00             100.00                              12.22         0.103343\n        mail string match                      ~F(d)            ((~ a) U ((F a) & r))   False       0.00              61.35                               5.24        52.270644\n        mail string match                      ~F(d)    (X (X ((X p) U ((X p) & p))))   False      87.60              93.04                              93.04        63.440962\n        mail string match                 F(a)->F(d)      ((q | p) U (X (X (q | p))))   False      87.60              51.52                              51.52        67.626487\n        mail string match                 F(a)->F(d)  ((~ a) U ((a U (G (~ a))) & a))   False      61.05              82.48                              82.48        59.714202\n"
    }
   ],
   "source": [
    "# read result\n",
    "df=pd.read_csv(\"output/result.csv\",header=None)\n",
    "df.columns=['target', \n",
    "            'query', \n",
    "            'explanation', \n",
    "            'status', \n",
    "            'rnn score', \n",
    "            'explanation score', \n",
    "            'explanation score on ground truth',\n",
    "            'extraction time'\n",
    "            ]\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}