made train set of size: 9549 , of which positive examples: 32
out of  19246  sequences 9729  are positive. (percent:  0.505507637950743 )
examples per length: [1, 4, 16, 64, 200, 204, 218, 242, 288, 352, 375, 403, 414, 426, 432, 432, 430, 433, 433, 432, 434, 433, 434, 431, 434, 433, 434, 434, 434, 433, 434, 434, 433, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434, 434]
size of train set: 17321
size of test set: 1925
The dy.parameter(...) call is now DEPRECATED.
        There is no longer need to explicitly add parameters to the computation graph.
        Any used parameter will be added automatically.
classification loss on last batch was: 0.00035971660327233475
testing on train set, i.e. test set is train set
rnn score against target on test set:                              1924 (99.94)
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10286927223205566  s
EQ test took  0.0007660388946533203  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.10381841659545898  s
EQ test took  0.022516965866088867  s


epsilon= 0.5 delta= 0.5 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.2337350845336914
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.016424179077148438
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.233735          None            None              []          0         0         0            2                    None                                    None               0.016424         True      0.5    0.5
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1167454719543457  s
EQ test took  0.0007004737854003906  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12198376655578613  s
EQ test took  0.022922515869140625  s


epsilon= 0.5 delta= 0.25 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.265514612197876
overall guided extraction time took: 0.03999999999996362
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.03740262985229492
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.265515          None            None              []          0         0         0            2                    None                                    None               0.037403         True      0.5   0.25
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12850141525268555  s
EQ test took  0.000637054443359375  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12241125106811523  s
EQ test took  0.02948474884033203  s


epsilon= 0.5 delta= 0.1 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.28487157821655273
overall guided extraction time took: 0.11000000000001364
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.1117711067199707
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.284872          None            None              []          0         0         0            2                    None                                    None               0.111771         True      0.5    0.1
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1230764389038086  s
EQ test took  0.0006604194641113281  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1220235824584961  s
EQ test took  0.033304452896118164  s


epsilon= 0.5 delta= 0.05 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.2829732894897461
overall guided extraction time took: 0.2599999999999909
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.26472020149230957
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.282973          None            None              []          0         0         0            2                    None                                    None                0.26472         True      0.5   0.05
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12103271484375  s
EQ test took  0.00066375732421875  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12208127975463867  s
EQ test took  0.033898353576660156  s


epsilon= 0.25 delta= 0.5 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.2814478874206543
overall guided extraction time took: 0.010000000000104592
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.016180992126464844
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.281448          None            None              []          0         0         0            2                    None                                    None               0.016181         True     0.25    0.5
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10353279113769531  s
EQ test took  0.0006413459777832031  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11722588539123535  s
EQ test took  0.04770231246948242  s


epsilon= 0.25 delta= 0.25 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.2727820873260498
overall guided extraction time took: 0.03999999999996362
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.036576271057128906
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.272782          None            None              []          0         0         0            2                    None                                    None               0.036576         True     0.25   0.25
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1097266674041748  s
EQ test took  0.0006561279296875  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11650753021240234  s
EQ test took  0.05657029151916504  s


epsilon= 0.25 delta= 0.1 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.28658223152160645
overall guided extraction time took: 0.11000000000001364
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.1093451976776123
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.286582          None            None              []          0         0         0            2                    None                                    None               0.109345         True     0.25    0.1
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10988235473632812  s
EQ test took  0.0006682872772216797  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11644482612609863  s
EQ test took  0.06551218032836914  s


epsilon= 0.25 delta= 0.05 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.29566407203674316
overall guided extraction time took: 0.2600000000001046
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.25844860076904297
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.295664          None            None              []          0         0         0            2                    None                                    None               0.258449         True     0.25   0.05
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11873602867126465  s
EQ test took  0.0006589889526367188  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.10914444923400879  s
EQ test took  0.07470822334289551  s


epsilon= 0.1 delta= 0.5 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.30707430839538574
overall guided extraction time took: 0.01999999999998181
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.016117095947265625
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.307074          None            None              []          0         0         0            2                    None                                    None               0.016117         True      0.1    0.5
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10956883430480957  s
EQ test took  0.0006325244903564453  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11708307266235352  s
EQ test took  0.09468460083007812  s


epsilon= 0.1 delta= 0.25 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.325087308883667
overall guided extraction time took: 0.03999999999996362
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.036805152893066406
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.325087          None            None              []          0         0         0            2                    None                                    None               0.036805         True      0.1   0.25
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10522723197937012  s
EQ test took  0.000659942626953125  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11817026138305664  s
EQ test took  0.11982178688049316  s


epsilon= 0.1 delta= 0.1 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.34767627716064453
overall guided extraction time took: 0.10999999999989996
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.10914421081542969
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.347676          None            None              []          0         0         0            2                    None                                    None               0.109144         True      0.1    0.1
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11739850044250488  s
EQ test took  0.0006573200225830078  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1123955249786377  s
EQ test took  0.14161944389343262  s


epsilon= 0.1 delta= 0.05 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.3758575916290283
overall guided extraction time took: 0.2599999999999909
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.25768256187438965
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.375858          None            None              []          0         0         0            2                    None                                    None               0.257683         True      0.1   0.05
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1045675277709961  s
EQ test took  0.0006766319274902344  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11573624610900879  s
EQ test took  0.13094758987426758  s


epsilon= 0.05 delta= 0.5 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.3555927276611328
overall guided extraction time took: 0.01999999999998181
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.01624298095703125
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.355593          None            None              []          0         0         0            2                    None                                    None               0.016243         True     0.05    0.5
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11533689498901367  s
EQ test took  0.0006809234619140625  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11077165603637695  s
EQ test took  0.17427921295166016  s


epsilon= 0.05 delta= 0.25 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.404987096786499
overall guided extraction time took: 0.04000000000007731
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.03778529167175293
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.404987          None            None              []          0         0         0            2                    None                                    None               0.037785         True     0.05   0.25
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10773563385009766  s
EQ test took  0.0006463527679443359  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11751770973205566  s
EQ test took  0.22234559059143066  s


epsilon= 0.05 delta= 0.1 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.4513392448425293
overall guided extraction time took: 0.11000000000001364
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.10878729820251465
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.451339          None            None              []          0         0         0            2                    None                                    None               0.108787         True     0.05    0.1
target: email match
query: false


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12007522583007812  s
EQ test took  0.0006511211395263672  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12832379341125488  s
EQ test took  0.27097439765930176  s


epsilon= 0.05 delta= 0.05 max_trace_length= 50
query: false
final ltl:  false

Time taken to extract ltl: 0.5239520072937012
overall guided extraction time took: 0.2699999999999818
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.2667708396911621
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
target  query explanation status  test accuracy rnn score explanation score explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states lstar explanation score lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  false       false   True          99.94      None              None                              None         0.523952          None            None              []          0         0         0            2                    None                                    None               0.266771         True     0.05   0.05
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1129605770111084  s
No positive counterexample found
EQ test took  0.0032012462615966797  s
new counterexample: a  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['a']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1149439811706543  s
EQ test took  0.001708984375  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['a']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x0)
Learning formula with depth 1
learned LTL formula: (~ a)
Learning took:  0.1724839210510254  s
No positive counterexample found
EQ test took  0.013698339462280273  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x1 | x0))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.350996732711792  s
No positive counterexample found
EQ test took  0.023023128509521484  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd', 'm']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! ((x0 | x1) | x2))
Learning formula with depth 3
learned LTL formula: (~ (m | (a | d)))
Learning took:  1.0450396537780762  s
No positive counterexample found
EQ test took  0.02343130111694336  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x1 | (x3 | x2)) | x0))
Learning formula with depth 4
learned LTL formula: (~ (a | (d | (m | p))))
Learning took:  7.5336668491363525  s
EQ test took  0.04447317123413086  s


epsilon= 0.5 delta= 0.5 max_trace_length= 50
query: true
final ltl:  (~ (a | (d | (m | p))))

Time taken to extract ltl: 9.448101043701172
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.010296344757080078
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 49.43
RNN matches ground truth: 99.98
Explanation matches ground truth: 49.44
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (a | (d | (m | p))))   True          99.94      99.98              49.43                              49.44         9.448101          None            None  [a, , d, m, p]          0         0         4            2                    49.43                                    49.44               0.010296         True      0.5    0.5
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1086268424987793  s
No positive counterexample found
EQ test took  0.003846883773803711  s
new counterexample: a  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['a']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11720895767211914  s
EQ test took  0.0016918182373046875  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['a']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x0)
Learning formula with depth 1
learned LTL formula: (~ a)
Learning took:  0.18303346633911133  s
No positive counterexample found
EQ test took  0.017362594604492188  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x2))
Learning formula with depth 2
learned LTL formula: (~ (a | m))
Learning took:  0.33997154235839844  s
No positive counterexample found
EQ test took  0.021226167678833008  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'm', 'd']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x0 | (x1 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (a | (d | m)))
Learning took:  1.0197408199310303  s
No positive counterexample found
EQ test took  0.032850027084350586  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'm', 'd', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! (x1 | (x0 | (x2 | x3))))
Learning formula with depth 4
learned LTL formula: (~ (d | (a | (m | p))))
Learning took:  8.260808229446411  s
EQ test took  0.044087886810302734  s


epsilon= 0.5 delta= 0.25 max_trace_length= 50
query: true
final ltl:  (~ (d | (a | (m | p))))

Time taken to extract ltl: 10.159886121749878
overall guided extraction time took: 0.029999999999972715
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.028802156448364258
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 49.43
RNN matches ground truth: 99.98
Explanation matches ground truth: 49.44
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (a | (m | p))))   True          99.94      99.98              49.43                              49.44        10.159886          None            None  [a, , m, d, p]          0         0         4            2                    49.43                                    49.44               0.028802         True      0.5   0.25
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10541248321533203  s
No positive counterexample found
EQ test took  0.0041697025299072266  s
new counterexample: a  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['a']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11675119400024414  s
EQ test took  0.0017099380493164062  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['a']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x0)
Learning formula with depth 1
learned LTL formula: (~ a)
Learning took:  0.16770195960998535  s
No positive counterexample found
EQ test took  0.01796889305114746  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x1 | x0))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.3448336124420166  s
EQ test took  0.041986703872680664  s


epsilon= 0.5 delta= 0.1 max_trace_length= 50
query: true
final ltl:  (~ (a | d))

Time taken to extract ltl: 0.8070650100708008
overall guided extraction time took: 0.10000000000002274
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.09232425689697266
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 75.32
RNN matches ground truth: 99.98
Explanation matches ground truth: 75.31
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (a | d))   True          99.94      99.98              75.32                              75.31         0.807065          None            None        [a, , d]          0         0         2            2                    49.43                                    49.44               0.092324         True      0.5    0.1
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12035250663757324  s
No positive counterexample found
EQ test took  0.004765748977661133  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1256122589111328  s
EQ test took  0.0016133785247802734  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.16380929946899414  s
No positive counterexample found
EQ test took  0.012752294540405273  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x1))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.3523092269897461  s
No positive counterexample found
EQ test took  0.04127025604248047  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'm']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x1 | (x2 | x0)))
Learning formula with depth 3
learned LTL formula: (~ (d | (a | m)))
Learning took:  1.0722377300262451  s
No positive counterexample found
EQ test took  0.04419422149658203  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x3 | (x0 | x2)) | x1))
Learning formula with depth 4
learned LTL formula: (~ (d | (p | (a | m))))
Learning took:  7.473876476287842  s
EQ test took  0.05663299560546875  s


epsilon= 0.5 delta= 0.05 max_trace_length= 50
query: true
final ltl:  (~ (d | (p | (a | m))))

Time taken to extract ltl: 9.478120565414429
overall guided extraction time took: 1.8999999999999773
generated counterexamples were: (format: (counterexample, counterexample generation time))
('pppdp', 0.22000000000002728)
('pmmdp', 0.2699999999999818)
('pamdpppa', 0.2999999999999545)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 1.9027905464172363
number of states of the dfa: 40
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'p'}, 'pppdpapp': {'a': 'a', 'd': 'pppdpappd', 'm': 'a', 'p': 'pppdpapp'}, 'ppp': {'a': 'pam', 'd': 'pppd', 'm': 'ppp', 'p': 'ppp'}, 'pamdpppampp': {'a': 'a', 'd': 'pamd', 'm': 'a', 'p': 'pmmd'}, 'pppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pppdp'}, 'pamdppppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamdppppdp'}, 'pamdppppm': {'a': 'pmaa', 'd': 'pamdppppmd', 'm': 'pam', 'p': 'pamdpppp'}, 'pamdppppap': {'a': 'a', 'd': 'pamdppppapd', 'm': 'pmma', 'p': 'pamdppp'}, 'pppdpa': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pppdpap'}, 'p': {'a': 'pa', 'd': 'a', 'm': 'pm', 'p': 'pp'}, 'pam': {'a': 'pmma', 'd': 'pamd', 'm': 'pam', 'p': 'pam'}, 'pamdppp': {'a': 'pamdpppa', 'd': 'pamdpppd', 'm': 'pamdppp', 'p': 'pamdpppp'}, 'pma': {'a': 'pmaa', 'd': 'pamd', 'm': 'pam', 'p': 'pam'}, 'pamd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamdp'}, 'pamdppppa': {'a': 'a', 'd': 'pamd', 'm': 'pmma', 'p': 'pamdppppap'}, 'pppdp': {'a': 'pppdpa', 'd': 'a', 'm': 'pmaa', 'p': 'pamdpp'}, 'pamdpppamp': {'a': 'a', 'd': 'pamd', 'm': 'a', 'p': 'pamdpppampp'}, 'pamdpppa': {'a': 'a', 'd': 'a', 'm': 'pamdpppam', 'p': 'pmma'}, 'pamdpppp': {'a': 'pamdppppa', 'd': 'pamdppppd', 'm': 'pamdppppm', 'p': 'pamdppppp'}, 'pp': {'a': 'pam', 'd': 'pmmd', 'm': 'ppp', 'p': 'ppp'}, 'pmma': {'a': 'pmaa', 'd': 'pamd', 'm': 'pma', 'p': 'pam'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'pppdpap': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pppdpapp'}, 'pamdppppmd': {'a': 'a', 'd': 'a', 'm': 'pamd', 'p': 'pamdpp'}, 'pmaam': {'a': 'a', 'd': 'pamd', 'm': 'pamdpppam', 'p': 'pppdpapp'}, 'pppdpappd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamdpppd'}, 'pmmd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pmmdp'}, 'pamdppppdp': {'a': 'a', 'd': 'a', 'm': 'pamdp', 'p': 'pamdppp'}, 'pmm': {'a': 'pmma', 'd': 'pmmd', 'm': 'pam', 'p': 'pam'}, 'pppa': {'a': 'pmma', 'd': 'pamd', 'm': 'pam', 'p': 'pam'}, 'pamdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamdpp'}, 'pa': {'a': 'pmaa', 'd': 'a', 'm': 'pam', 'p': 'pam'}, 'pmmdp': {'a': 'a', 'd': 'a', 'm': 'pmmdp', 'p': 'pmaa'}, 'pamdpppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamdp'}, 'pm': {'a': 'pma', 'd': 'a', 'm': 'pmm', 'p': 'ppp'}, 'pamdpppam': {'a': 'a', 'd': 'pmmd', 'm': 'a', 'p': 'pamdpppamp'}, 'pamdppppapd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamdppppdp'}, 'pmaa': {'a': 'a', 'd': 'a', 'm': 'pmaam', 'p': 'pmma'}, 'pamdpp': {'a': 'a', 'd': 'a', 'm': 'pmaa', 'p': 'pamdppp'}, 'pamdppppp': {'a': 'pamdppppa', 'd': 'pamdppppdp', 'm': 'pamdppppp', 'p': 'pamdppppp'}}
Explanation matches RNN: 49.43
RNN matches ground truth: 99.98
Explanation matches ground truth: 49.44
Lstar matches RNN: 99.97
Lstar matches ground truth: 99.96
target query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (p | (a | m))))   True          99.94      99.98              49.43                              49.44         9.478121          None            None  [d, , a, m, p]          0         0         4           40                    99.97                                    99.96               1.902791         True      0.5   0.05
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12388205528259277  s
No positive counterexample found
EQ test took  0.004138469696044922  s
new counterexample: a  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['a']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1232602596282959  s
EQ test took  0.001661539077758789  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['a']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x0)
Learning formula with depth 1
learned LTL formula: (~ a)
Learning took:  0.17575502395629883  s
No positive counterexample found
EQ test took  0.012711763381958008  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x3 | x0))
Learning formula with depth 2
learned LTL formula: (~ (a | p))
Learning took:  0.34487485885620117  s
EQ test took  0.01989126205444336  s
new counterexample: pammdp  should be accepted by implementation


positive traces---> 
['', 'pammdp']


negative traces---> 
['a', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
Before normalization: ((x3 | x0) -> (X x0))
Learning formula with depth 2
learned LTL formula: ((a | p) -> (X a))
Learning took:  0.8314013481140137  s
EQ test took  0.027362346649169922  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['', 'pammdp']


negative traces---> 
['a', 'p', 'd']



4  iteration complete



start formula depth: 5
increasing formula depth to  6
increasing formula depth to  7
Before normalization: ((x0 | (x3 | x1)) -> (X x0))
Learning formula with depth 3
learned LTL formula: ((a | (d | p)) -> (X a))
Learning took:  3.491472005844116  s
EQ test took  0.04546809196472168  s
new counterexample: pmmampdp  should be accepted by implementation


positive traces---> 
['', 'pammdp', 'pmmampdp']


negative traces---> 
['a', 'p', 'd']



5  iteration complete



start formula depth: 7
Before normalization: (! (x1 | (G (x0 | x3))))
Learning formula with depth 4
learned LTL formula: (~ (d | (G (a | p))))
Learning took:  3.087846517562866  s
EQ test took  0.02959728240966797  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['', 'pammdp', 'pmmampdp']


negative traces---> 
['a', 'p', 'd', 'm']



6  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: (! ((G (x3 | x2)) | (x0 | x1)))
Learning formula with depth 4
learned LTL formula: (~ ((a | d) | (G (m | p))))
Learning took:  102.4622094631195  s
No positive counterexample found
EQ test took  0.06689262390136719  s
new counterexample: pa  should be rejected by implementation


positive traces---> 
['', 'pammdp', 'pmmampdp']


negative traces---> 
['a', 'p', 'd', 'm', 'pa']



7  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (G (x0 | (x2 | x3)))))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (a | (m | p)))))
Learning took:  20.56604290008545  s
No positive counterexample found
EQ test took  0.04077792167663574  s
new counterexample: ad  should be rejected by implementation


positive traces---> 
['', 'pammdp', 'pmmampdp']


negative traces---> 
['a', 'p', 'd', 'm', 'pa', 'ad']



8  iteration complete



start formula depth: 9
Before normalization: (! (x2 | (G (x0 | (x1 | x3)))))
Learning formula with depth 5
learned LTL formula: (~ (m | (G (a | (d | p)))))
Learning took:  36.27868914604187  s
EQ test took  0.019320249557495117  s
new counterexample: papdp  should be accepted by implementation


positive traces---> 
['', 'pammdp', 'pmmampdp', 'papdp']


negative traces---> 
['a', 'p', 'd', 'm', 'pa', 'ad']



9  iteration complete





epsilon= 0.25 delta= 0.5 max_trace_length= 50
query: true
final ltl:  (~ (m | (G (a | (d | p)))))
incomplete formula
Number of samples: 31
Number of counterexamples returned: 16
7068027.925525421 1.3476159495969127

Time taken to extract ltl: 401.11383533477783
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.01697564125061035
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 63.83
RNN matches ground truth: 99.98
Explanation matches ground truth: 63.82
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                                  counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (m | (G (a | (d | p)))))  False          99.94      99.98              63.83                              63.82       401.113835   7.068028e+06         1.347616  [a, , p, pammdp, d, pmmampdp, m, pa, ad, papdp]          0         0         5            2                    49.43                                    49.44               0.016976         True     0.25    0.5
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11235499382019043  s
No positive counterexample found
EQ test took  0.005427122116088867  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11499190330505371  s
EQ test took  0.001678466796875  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.17497968673706055  s
No positive counterexample found
EQ test took  0.024292469024658203  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x2 | x1))
Learning formula with depth 2
learned LTL formula: (~ (d | m))
Learning took:  0.35837221145629883  s
No positive counterexample found
EQ test took  0.03607797622680664  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x3 | (x1 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (p | (d | m)))
Learning took:  1.1191484928131104  s
No positive counterexample found
EQ test took  0.05320882797241211  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'p', 'a']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x3 | (x0 | x2)) | x1))
Learning formula with depth 4
learned LTL formula: (~ (d | (p | (a | m))))
Learning took:  10.020328283309937  s
EQ test took  0.0797426700592041  s
new counterexample: ppadp  should be accepted by implementation


positive traces---> 
['', 'ppadp']


negative traces---> 
['d', 'm', 'p', 'a']



5  iteration complete



start formula depth: 8
increasing formula depth to  9
Before normalization: ((x2 | (x3 | (x1 | x0))) -> (X x3))
Learning formula with depth 4
learned LTL formula: ((m | (p | (a | d))) -> (X p))
Learning took:  58.76309275627136  s
EQ test took  0.0720221996307373  s
new counterexample: pp  should be rejected by implementation


positive traces---> 
['', 'ppadp']


negative traces---> 
['d', 'm', 'p', 'a', 'pp']



6  iteration complete



start formula depth: 9
Before normalization: (! (((G x3) | (x0 | x1)) | x2))
Learning formula with depth 4
learned LTL formula: (~ (m | ((a | d) | (G p))))
Learning took:  6.283384084701538  s
No positive counterexample found
EQ test took  0.07107830047607422  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'ppadp']


negative traces---> 
['d', 'm', 'p', 'a', 'pp', 'pd']



7  iteration complete



start formula depth: 9
Before normalization: (! ((G (x3 | x1)) | (x0 | x2)))
Learning formula with depth 4
learned LTL formula: (~ ((a | m) | (G (d | p))))
Learning took:  10.149802923202515  s
EQ test took  0.047800302505493164  s
new counterexample: mppppdpp  should be accepted by implementation


positive traces---> 
['', 'ppadp', 'mppppdpp']


negative traces---> 
['d', 'm', 'p', 'a', 'pp', 'pd']



8  iteration complete





epsilon= 0.25 delta= 0.25 max_trace_length= 50
query: true
final ltl:  (~ ((a | m) | (G (d | p))))
incomplete formula
Number of samples: 31
Number of counterexamples returned: 16
7068027.925525421 1.3938257616342424

Time taken to extract ltl: 401.11446237564087
overall guided extraction time took: 0.029999999999972715
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.03554558753967285
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 75.38
RNN matches ground truth: 99.98
Explanation matches ground truth: 75.37
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                          counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ ((a | m) | (G (d | p))))  False          99.94      99.98              75.38                              75.37       401.114462   7.068028e+06         1.393826  [d, , m, p, a, ppadp, pp, pd, mppppdpp]          0         0         4            2                    49.43                                    49.44               0.035546         True     0.25   0.25
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11494970321655273  s
No positive counterexample found
EQ test took  0.006173610687255859  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12285518646240234  s
EQ test took  0.001664876937866211  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.1798405647277832  s
No positive counterexample found
EQ test took  0.025201797485351562  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x2 | x1))
Learning formula with depth 2
learned LTL formula: (~ (d | m))
Learning took:  0.33740234375  s
No positive counterexample found
EQ test took  0.03999519348144531  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x3 | (x1 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (p | (d | m)))
Learning took:  1.1184825897216797  s
EQ test took  0.044071197509765625  s
new counterexample: pmapdp  should be accepted by implementation


positive traces---> 
['', 'pmapdp']


negative traces---> 
['d', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
Before normalization: ((x2 | x3) U (! (x1 | (x2 | x3))))
Learning formula with depth 4
learned LTL formula: ((m | p) U (~ (d | (m | p))))
Learning took:  3.2340800762176514  s
EQ test took  0.05830192565917969  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'pmapdp']


negative traces---> 
['d', 'm', 'p', 'a']



5  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((x2 | (x3 | (x1 | x0))) -> (X x2))
Learning formula with depth 4
learned LTL formula: ((m | (p | (a | d))) -> (X m))
Learning took:  71.11317086219788  s
No positive counterexample found
EQ test took  0.07238435745239258  s
new counterexample: mm  should be rejected by implementation


positive traces---> 
['', 'pmapdp']


negative traces---> 
['d', 'm', 'p', 'a', 'mm']



6  iteration complete



start formula depth: 9
Before normalization: (! (((G x3) | (x0 | x2)) | x1))
Learning formula with depth 4
learned LTL formula: (~ (d | ((a | m) | (G p))))
Learning took:  4.292406797409058  s
No positive counterexample found
EQ test took  0.08662676811218262  s
new counterexample: pm  should be rejected by implementation


positive traces---> 
['', 'pmapdp']


negative traces---> 
['d', 'm', 'p', 'a', 'mm', 'pm']



7  iteration complete



start formula depth: 9
Before normalization: (! ((G ((x3 | x2) | x1)) | x0))
Learning formula with depth 5
learned LTL formula: (~ (a | (G (d | (m | p)))))
Learning took:  8.100919961929321  s
No positive counterexample found
EQ test took  0.04409384727478027  s
new counterexample: pa  should be rejected by implementation


positive traces---> 
['', 'pmapdp']


negative traces---> 
['d', 'm', 'p', 'a', 'mm', 'pm', 'pa']



8  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (G ((x3 | x0) | x2))))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (m | (a | p)))))
Learning took:  25.835070848464966  s
No positive counterexample found
EQ test took  0.05277872085571289  s
new counterexample: md  should be rejected by implementation


positive traces---> 
['', 'pmapdp']


negative traces---> 
['d', 'm', 'p', 'a', 'mm', 'pm', 'pa', 'md']



9  iteration complete





epsilon= 0.25 delta= 0.1 max_trace_length= 50
query: true
final ltl:  (~ (d | (G (m | (a | p)))))
incomplete formula
Number of samples: 37
Number of counterexamples returned: 27
28592678.734222483 2.1971246347699624

Time taken to extract ltl: 401.11369347572327
overall guided extraction time took: 0.10000000000002274
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.10283613204956055
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 64.39
RNN matches ground truth: 99.98
Explanation matches ground truth: 64.37
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                         counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (G (m | (a | p)))))  False          99.94      99.98              64.39                              64.37       401.113693   2.859268e+07         2.197125  [d, , m, p, pmapdp, a, mm, pm, pa, md]          0         0         5            2                    49.43                                    49.44               0.102836         True     0.25    0.1
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11384296417236328  s
No positive counterexample found
EQ test took  0.007111310958862305  s
new counterexample: p  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['p']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1022489070892334  s
EQ test took  0.0015990734100341797  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['p']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x3)
Learning formula with depth 1
learned LTL formula: (~ p)
Learning took:  0.17137455940246582  s
EQ test took  0.014656305313110352  s
new counterexample: pppdp  should be accepted by implementation


positive traces---> 
['', 'pppdp']


negative traces---> 
['p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
Before normalization: (x3 -> (X x3))
Learning formula with depth 2
learned LTL formula: (p -> (X p))
Learning took:  0.26381778717041016  s
EQ test took  0.0357663631439209  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['', 'pppdp']


negative traces---> 
['p', 'm']



3  iteration complete



start formula depth: 3
increasing formula depth to  4
increasing formula depth to  5
Before normalization: ((x3 | x2) -> (X x3))
Learning formula with depth 2
learned LTL formula: ((m | p) -> (X p))
Learning took:  0.845890998840332  s
EQ test took  0.01890277862548828  s
new counterexample: pmamdp  should be accepted by implementation


positive traces---> 
['', 'pppdp', 'pmamdp']


negative traces---> 
['p', 'm']



4  iteration complete



start formula depth: 5
Before normalization: ((x3 | x2) -> (X (x3 | x2)))
Learning formula with depth 3
learned LTL formula: ((m | p) -> (X (m | p)))
Learning took:  0.8473308086395264  s
EQ test took  0.0449070930480957  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pmamdp']


negative traces---> 
['p', 'm', 'd']



5  iteration complete



start formula depth: 5
increasing formula depth to  6
increasing formula depth to  7
Before normalization: (((x2 | x3) | x1) -> (X (x2 | x3)))
Learning formula with depth 3
learned LTL formula: ((d | (m | p)) -> (X (m | p)))
Learning took:  5.026553153991699  s
EQ test took  0.03890681266784668  s
new counterexample: paadpp  should be accepted by implementation


positive traces---> 
['', 'pppdp', 'pmamdp', 'paadpp']


negative traces---> 
['p', 'm', 'd']



6  iteration complete



start formula depth: 7
Before normalization: (! ((G (x3 | x2)) | x1))
Learning formula with depth 4
learned LTL formula: (~ (d | (G (m | p))))
Learning took:  3.7547764778137207  s
EQ test took  0.04610586166381836  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pmamdp', 'paadpp']


negative traces---> 
['p', 'm', 'd', 'a']



7  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((((x0 | x2) | x3) | x1) -> (X ((x0 | x2) | x3)))
Learning formula with depth 4
learned LTL formula: ((d | (p | (a | m))) -> (X (p | (a | m))))
Learning took:  120.35675239562988  s
No positive counterexample found
EQ test took  0.04409503936767578  s
new counterexample: aa  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pmamdp', 'paadpp']


negative traces---> 
['p', 'm', 'd', 'a', 'aa']



8  iteration complete



start formula depth: 9
Before normalization: (! ((x2 | (G (x3 | x0))) | x1))
Learning formula with depth 5
learned LTL formula: (~ (d | (m | (G (a | p)))))
Learning took:  10.283256769180298  s
No positive counterexample found
EQ test took  0.064971923828125  s
new counterexample: ad  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pmamdp', 'paadpp']


negative traces---> 
['p', 'm', 'd', 'a', 'aa', 'ad']



9  iteration complete



start formula depth: 9
Before normalization: (! ((x2 | (x0 | (G x3))) | x1))
Learning formula with depth 5
learned LTL formula: (~ (d | (m | (a | (G p)))))
Learning took:  14.13774299621582  s
No positive counterexample found
EQ test took  0.12292909622192383  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pmamdp', 'paadpp']


negative traces---> 
['p', 'm', 'd', 'a', 'aa', 'ad', 'pd']



10  iteration complete





epsilon= 0.25 delta= 0.05 max_trace_length= 50
query: true
final ltl:  (~ (d | (m | (a | (G p)))))
incomplete formula
Number of samples: 43
Number of counterexamples returned: 5
72.05222174624085 0.4413979850446518

Time taken to extract ltl: 401.1135091781616
overall guided extraction time took: 1.0299999999999727
generated counterexamples were: (format: (counterexample, counterexample generation time))
('paapdp', 0.22000000000002728)
('pmmmdp', 0.25)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 1.037475347518921
number of states of the dfa: 20
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'p'}, 'pmmmdpp': {'a': 'pmmmdpm', 'd': 'a', 'm': 'paap', 'p': 'pmmmdppp'}, 'pmmmdpppm': {'a': 'paap', 'd': 'pmmmd', 'm': 'pmmm', 'p': 'pmmmdpppm'}, 'p': {'a': 'pa', 'd': 'a', 'm': 'pm', 'p': 'pmm'}, 'paapdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pmmmdp'}, 'pam': {'a': 'paap', 'd': 'paapd', 'm': 'pam', 'p': 'pmmm'}, 'pma': {'a': 'paa', 'd': 'paapd', 'm': 'pam', 'p': 'pam'}, 'pmmmdppp': {'a': 'paap', 'd': 'pmmmdpppd', 'm': 'pmmmdpppm', 'p': 'pmmmdpppm'}, 'pmmmdpppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'paapdp'}, 'paa': {'a': 'a', 'd': 'a', 'm': 'paap', 'p': 'paap'}, 'paap': {'a': 'a', 'd': 'paapd', 'm': 'paap', 'p': 'paap'}, 'pmmmdp': {'a': 'a', 'd': 'a', 'm': 'pmmmdpm', 'p': 'pmmmdpp'}, 'pmmmdpm': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'paap'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'pmmm': {'a': 'paap', 'd': 'pmmmd', 'm': 'pmmm', 'p': 'pmmm'}, 'pmm': {'a': 'paap', 'd': 'a', 'm': 'pmmm', 'p': 'pmmm'}, 'pmmmd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pmmmdp'}, 'pa': {'a': 'paa', 'd': 'a', 'm': 'pam', 'p': 'pam'}, 'paapd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'paapdp'}, 'pm': {'a': 'pma', 'd': 'a', 'm': 'pmm', 'p': 'pmmm'}}
Explanation matches RNN: 87.63
RNN matches ground truth: 99.98
Explanation matches ground truth: 87.61
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.98
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                                    counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (m | (a | (G p)))))  False          99.94      99.98              87.63                              87.61       401.113509      72.052222         0.441398  [p, , pppdp, m, pmamdp, d, paadpp, a, aa, ad, pd]          0         0         5           20                    100.0                                    99.98               1.037475         True     0.25   0.05
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11789059638977051  s
No positive counterexample found
EQ test took  0.006745100021362305  s
new counterexample: m  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['m']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11109280586242676  s
EQ test took  0.0015861988067626953  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['m']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x2)
Learning formula with depth 1
learned LTL formula: (~ m)
Learning took:  0.16698837280273438  s
EQ test took  0.017847061157226562  s
new counterexample: mppppdpp  should be accepted by implementation


positive traces---> 
['', 'mppppdpp']


negative traces---> 
['m']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
Before normalization: (x2 U (! x2))
Learning formula with depth 2
learned LTL formula: (m U (~ m))
Learning took:  0.3074355125427246  s
EQ test took  0.01228952407836914  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['', 'mppppdpp']


negative traces---> 
['m', 'p']



3  iteration complete



start formula depth: 3
increasing formula depth to  4
increasing formula depth to  5
Before normalization: ((x3 | x2) -> (X x3))
Learning formula with depth 2
learned LTL formula: ((m | p) -> (X p))
Learning took:  1.1964313983917236  s
No positive counterexample found
EQ test took  0.06223297119140625  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'mppppdpp']


negative traces---> 
['m', 'p', 'a']



4  iteration complete



start formula depth: 5
increasing formula depth to  6
increasing formula depth to  7
Before normalization: (((x3 | x0) | x2) -> (X x3))
Learning formula with depth 3
learned LTL formula: ((m | (a | p)) -> (X p))
Learning took:  4.219626188278198  s
EQ test took  0.06100177764892578  s
new counterexample: papdp  should be accepted by implementation


positive traces---> 
['', 'mppppdpp', 'papdp']


negative traces---> 
['m', 'p', 'a']



5  iteration complete



start formula depth: 7
Before normalization: ((x2 | (x3 | x0)) -> (X (x3 | x0)))
Learning formula with depth 3
learned LTL formula: ((m | (a | p)) -> (X (a | p)))
Learning took:  2.6969785690307617  s
EQ test took  0.022690534591674805  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['', 'mppppdpp', 'papdp']


negative traces---> 
['m', 'p', 'a', 'd']



6  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((((x0 | x3) | x1) | x2) -> (X (x0 | x3)))
Learning formula with depth 4
learned LTL formula: ((m | (d | (a | p))) -> (X (a | p)))
Learning took:  78.95357632637024  s
No positive counterexample found
EQ test took  0.10969662666320801  s
new counterexample: pa  should be rejected by implementation


positive traces---> 
['', 'mppppdpp', 'papdp']


negative traces---> 
['m', 'p', 'a', 'd', 'pa']



7  iteration complete



start formula depth: 9
Before normalization: (! ((G ((x2 | x0) | x3)) | x1))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (p | (a | m)))))
Learning took:  13.714059352874756  s
No positive counterexample found
EQ test took  0.0862126350402832  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'mppppdpp', 'papdp']


negative traces---> 
['m', 'p', 'a', 'd', 'pa', 'pd']



8  iteration complete





epsilon= 0.1 delta= 0.5 max_trace_length= 50
query: true
final ltl:  (~ (d | (G (p | (a | m)))))
incomplete formula
Number of samples: 70
Number of counterexamples returned: 52
3834295287382813.0 2.1319949575125032

Time taken to extract ltl: 401.11479592323303
overall guided extraction time took: 0.2900000000000773
generated counterexamples were: (format: (counterexample, counterexample generation time))
('papaapdpp', 0.009999999999990905)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.2886009216308594
number of states of the dfa: 15
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'p'}, 'pap': {'a': 'papa', 'd': 'papaapdp', 'm': 'pap', 'p': 'pap'}, 'papa': {'a': 'papaa', 'd': 'papaapdp', 'm': 'papa', 'p': 'papa'}, 'p': {'a': 'pa', 'd': 'a', 'm': 'pa', 'p': 'pa'}, 'papaap': {'a': 'a', 'd': 'papaapd', 'm': 'a', 'p': 'papaap'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'papaapdppp': {'a': 'a', 'd': 'a', 'm': 'papaa', 'p': 'papaapdpppp'}, 'papaa': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papaap'}, 'pa': {'a': 'paa', 'd': 'a', 'm': 'pap', 'p': 'pap'}, 'papaapd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papaapdp'}, 'papaapdpppp': {'a': 'a', 'd': 'papaapd', 'm': 'papaap', 'p': 'papaapdppppp'}, 'papaapdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papaapdpp'}, 'papaapdppppp': {'a': 'a', 'd': 'papaapd', 'm': 'papaap', 'p': 'papa'}, 'paa': {'a': 'papaa', 'd': 'papaapd', 'm': 'papa', 'p': 'papa'}, 'papaapdpp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papaapdppp'}}
Explanation matches RNN: 64.39
RNN matches ground truth: 99.98
Explanation matches ground truth: 64.37
Lstar matches RNN: 74.51
Lstar matches ground truth: 74.51
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                          counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (G (p | (a | m)))))  False          99.94      99.98              64.39                              64.37       401.114796   3.834295e+15         2.131995  [m, , mppppdpp, p, a, papdp, d, pa, pd]          0         0         5           15                    74.51                                    74.51               0.288601         True      0.1    0.5
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11250996589660645  s
No positive counterexample found
EQ test took  0.008964061737060547  s
new counterexample: m  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['m']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.10720586776733398  s
EQ test took  0.0016241073608398438  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['m']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x2)
Learning formula with depth 1
learned LTL formula: (~ m)
Learning took:  0.16738080978393555  s
No positive counterexample found
EQ test took  0.0395054817199707  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'a']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x2))
Learning formula with depth 2
learned LTL formula: (~ (a | m))
Learning took:  0.3276090621948242  s
No positive counterexample found
EQ test took  0.07836270332336426  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'a', 'd']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x1 | (x0 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (d | (a | m)))
Learning took:  0.9847478866577148  s
No positive counterexample found
EQ test took  0.11618709564208984  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'a', 'd', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! (x3 | ((x1 | x2) | x0)))
Learning formula with depth 4
learned LTL formula: (~ (p | (a | (d | m))))
Learning took:  7.822035074234009  s
EQ test took  0.1820533275604248  s


epsilon= 0.1 delta= 0.25 max_trace_length= 50
query: true
final ltl:  (~ (p | (a | (d | m))))

Time taken to extract ltl: 9.956380367279053
overall guided extraction time took: 0.029999999999972715
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.028893232345581055
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 49.43
RNN matches ground truth: 99.98
Explanation matches ground truth: 49.44
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (p | (a | (d | m))))   True          99.94      99.98              49.43                              49.44          9.95638          None            None  [m, , a, d, p]          0         0         4            2                    49.43                                    49.44               0.028893         True      0.1   0.25
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1261739730834961  s
No positive counterexample found
EQ test took  0.011816263198852539  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11324095726013184  s
EQ test took  0.0016925334930419922  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.17232871055603027  s
No positive counterexample found
EQ test took  0.04008984565734863  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x1))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.335430383682251  s
No positive counterexample found
EQ test took  0.08351969718933105  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'm']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x1 | (x2 | x0)))
Learning formula with depth 3
learned LTL formula: (~ (d | (a | m)))
Learning took:  1.1112239360809326  s
No positive counterexample found
EQ test took  0.1329054832458496  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x3 | (x0 | x2)) | x1))
Learning formula with depth 4
learned LTL formula: (~ (d | (p | (a | m))))
Learning took:  7.542572259902954  s
EQ test took  0.20787334442138672  s


epsilon= 0.1 delta= 0.1 max_trace_length= 50
query: true
final ltl:  (~ (d | (p | (a | m))))

Time taken to extract ltl: 9.889342069625854
overall guided extraction time took: 0.08000000000004093
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.0896458625793457
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 49.43
RNN matches ground truth: 99.98
Explanation matches ground truth: 49.44
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (p | (a | m))))   True          99.94      99.98              49.43                              49.44         9.889342          None            None  [d, , a, m, p]          0         0         4            2                    49.43                                    49.44               0.089646         True      0.1    0.1
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10905098915100098  s
No positive counterexample found
EQ test took  0.013816595077514648  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11007428169250488  s
EQ test took  0.0016186237335205078  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.17111945152282715  s
No positive counterexample found
EQ test took  0.05696296691894531  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x2 | x1))
Learning formula with depth 2
learned LTL formula: (~ (d | m))
Learning took:  0.33424830436706543  s
No positive counterexample found
EQ test took  0.10003089904785156  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x3 | (x1 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (p | (d | m)))
Learning took:  1.143404483795166  s
EQ test took  0.11912727355957031  s
new counterexample: pamdp  should be accepted by implementation


positive traces---> 
['', 'pamdp']


negative traces---> 
['d', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
Before normalization: (x3 U (! (x1 | (x3 | x2))))
Learning formula with depth 4
learned LTL formula: (p U (~ (d | (m | p))))
Learning took:  3.0798277854919434  s
EQ test took  0.06404972076416016  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'pamdp']


negative traces---> 
['d', 'm', 'p', 'a']



5  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((x2 | ((x0 | x3) | x1)) -> (X x0))
Learning formula with depth 4
learned LTL formula: ((m | (d | (a | p))) -> (X a))
Learning took:  76.69201731681824  s
EQ test took  0.06067228317260742  s
new counterexample: ppmmdp  should be accepted by implementation


positive traces---> 
['', 'pamdp', 'ppmmdp']


negative traces---> 
['d', 'm', 'p', 'a']



6  iteration complete



start formula depth: 9
Before normalization: (((x1 | (x3 | x0)) | x2) -> (X (x1 | (x3 | x0))))
Learning formula with depth 4
learned LTL formula: ((m | (d | (a | p))) -> (X (d | (a | p))))
Learning took:  7.1922688484191895  s
EQ test took  0.0825190544128418  s
new counterexample: dd  should be rejected by implementation


positive traces---> 
['', 'pamdp', 'ppmmdp']


negative traces---> 
['d', 'm', 'p', 'a', 'dd']



7  iteration complete



start formula depth: 9
Before normalization: (! (((G x3) | (x2 | x1)) | x0))
Learning formula with depth 4
learned LTL formula: (~ (a | ((d | m) | (G p))))
Learning took:  6.7373340129852295  s
No positive counterexample found
EQ test took  0.2095019817352295  s
new counterexample: pm  should be rejected by implementation


positive traces---> 
['', 'pamdp', 'ppmmdp']


negative traces---> 
['d', 'm', 'p', 'a', 'dd', 'pm']



8  iteration complete



start formula depth: 9
Before normalization: (((x0 | x3) | (x2 | x1)) -> (X (x0 | x3)))
Learning formula with depth 3
learned LTL formula: (((a | p) | (d | m)) -> (X (a | p)))
Learning took:  9.231175899505615  s
EQ test took  0.046965837478637695  s
new counterexample: pmpapapmmdp  should be accepted by implementation


positive traces---> 
['', 'pamdp', 'ppmmdp', 'pmpapapmmdp']


negative traces---> 
['d', 'm', 'p', 'a', 'dd', 'pm']



9  iteration complete



start formula depth: 9
Before normalization: (! ((x1 | (G (x3 | x2))) | x0))
Learning formula with depth 5
learned LTL formula: (~ (a | (d | (G (m | p)))))
Learning took:  31.589015007019043  s
EQ test took  0.17604398727416992  s
new counterexample: pa  should be rejected by implementation


positive traces---> 
['', 'pamdp', 'ppmmdp', 'pmpapapmmdp']


negative traces---> 
['d', 'm', 'p', 'a', 'dd', 'pm', 'pa']



10  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (G (x0 | (x2 | x3)))))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (a | (m | p)))))
Learning took:  21.06590461730957  s
No positive counterexample found
EQ test took  0.14438152313232422  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'pamdp', 'ppmmdp', 'pmpapapmmdp']


negative traces---> 
['d', 'm', 'p', 'a', 'dd', 'pm', 'pa', 'pd']



11  iteration complete





epsilon= 0.1 delta= 0.05 max_trace_length= 50
query: true
final ltl:  (~ (d | (G (a | (m | p)))))
incomplete formula
Number of samples: 114
Number of counterexamples returned: 80
4.0174305351788416e+27 2.0575344797739463

Time taken to extract ltl: 401.11471819877625
overall guided extraction time took: 3.4700000000000273
generated counterexamples were: (format: (counterexample, counterexample generation time))
('ppmpdp', 0.22000000000002728)
('pmmaaapamdp', 0.2700000000000955)
('papppmpdppa', 0.1999999999999318)

Time taken to extract lstar-dfa: 3.473198890686035
number of states of the dfa: 68
returned flag: True
transitions:->
{'': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'p'}, 'ppmppma': {'a': 'pmmaa', 'd': 'ppmpd', 'm': 'papppmp', 'p': 'papppmp'}, 'ppmpdpppm': {'a': 'papppmpa', 'd': 'ppmpd', 'm': 'ppmpdpppmm', 'p': 'ppmpdpppmp'}, 'ppmpdpppmpapppp': {'a': 'pmmaam', 'd': 'papppmpd', 'm': 'ppmpdpppmpappppm', 'p': 'ppmpdpppmpapppp'}, 'papppmp': {'a': 'papppmpa', 'd': 'papppmpd', 'm': 'papppmp', 'p': 'papppmp'}, 'ppmppppdp': {'a': 'pmmaam', 'd': 'ppmpd', 'm': 'ppmpdp', 'p': 'papppmpdpp'}, 'ppm': {'a': 'ppma', 'd': 'ppmpd', 'm': 'ppmp', 'p': 'ppmp'}, 'ppmppm': {'a': 'ppmppma', 'd': 'papppmpd', 'm': 'ppmppm', 'p': 'ppmppm'}, 'pmmaampp': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'ppmppppd': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'ppmpd', 'p': 'ppmppppdp'}, 'ppma': {'a': 'papppmpa', 'd': 'ppmpd', 'm': 'pappp', 'p': 'pappp'}, 'papppmpdpp': {'a': 'papppmpdppa', 'd': 'ppmpdpppd', 'm': 'papppmpdppm', 'p': 'papppmpdppp'}, 'ppmpdpppmpappp': {'a': 'pmmaam', 'd': 'ppmpd', 'm': 'ppmpdpppmpapmm', 'p': 'ppmpdpppmpapppp'}, 'ppmpdpppmpapmp': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'ppmpdpppmpapm', 'p': 'ppmpdpppmpapmp'}, 'pmmaam': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaamp'}, 'ppmpp': {'a': 'papppm', 'd': 'ppmpd', 'm': 'ppmppm', 'p': 'ppmppp'}, 'pmmaaapamd': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'ppmpdpppp': {'a': 'papppmpdppm', 'd': 'papppmpdppa', 'm': 'ppmpdpppmp', 'p': 'ppmpdpppp'}, 'papppmpdpppmp': {'a': 'papppmpdpppmpa', 'd': 'papppmpdppa', 'm': 'papppmpdpppmpm', 'p': 'papppmpdpppmp'}, 'ppmpdpppmp': {'a': 'ppmpdpppmpa', 'd': 'papppmpdppa', 'm': 'ppmpdpppmm', 'p': 'ppmpdpppmp'}, 'p': {'a': 'pa', 'd': 'pmmaaapamd', 'm': 'pm', 'p': 'pp'}, 'ppmpdpppmpappppmp': {'a': 'pmmaa', 'd': 'papppmpd', 'm': 'ppmpdpppmpappppm', 'p': 'ppmpdpppmpapppp'}, 'pma': {'a': 'pmmaa', 'd': 'ppmpd', 'm': 'ppma', 'p': 'ppma'}, 'ppmpdpppmpapm': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'ppmpdpppmpapmm', 'p': 'ppmpdpppmpapmp'}, 'ppmpdpppmpapmm': {'a': 'pmmaaapamd', 'd': 'papppmpd', 'm': 'ppmpdpppmpapmm', 'p': 'ppmpdpppmpapm'}, 'pmmam': {'a': 'pmmaa', 'd': 'ppmpd', 'm': 'papppm', 'p': 'papppm'}, 'ppmpdpppmpa': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'papppmpa', 'p': 'ppmpdpppmpap'}, 'pmmaamp': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'pmmaaapamd', 'p': 'pmmaampp'}, 'ppmpdppp': {'a': 'papppmpdp', 'd': 'ppmpdpppd', 'm': 'ppmpdpppm', 'p': 'ppmpdpppp'}, 'pp': {'a': 'ppma', 'd': 'pmmaaapamd', 'm': 'ppm', 'p': 'ppm'}, 'pmmaaapam': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'ppmpdpppmm': {'a': 'ppmpdpppmpa', 'd': 'papppmpd', 'm': 'papppmp', 'p': 'ppmpdpppmp'}, 'ppmpdp': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaa', 'p': 'ppmpdpp'}, 'pmma': {'a': 'pmmaa', 'd': 'ppmpd', 'm': 'pmmam', 'p': 'pappp'}, 'pmmaa': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaam', 'p': 'papppmpa'}, 'pappp': {'a': 'papppmpa', 'd': 'ppmpd', 'm': 'papppm', 'p': 'papppm'}, 'a': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'pmmaaa': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'papp': {'a': 'papppmpa', 'd': 'ppmpd', 'm': 'pappp', 'p': 'pappp'}, 'ppmpdpppmpappppm': {'a': 'pmmaam', 'd': 'papppmpd', 'm': 'ppmpdpppmpappppm', 'p': 'ppmpdpppmpappppmp'}, 'ppmpdpp': {'a': 'pmmaa', 'd': 'ppmpdpppd', 'm': 'ppmpdpp', 'p': 'ppmpdppp'}, 'papppmpd': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'papppmpdp'}, 'ppmp': {'a': 'pappp', 'd': 'ppmpd', 'm': 'ppmpm', 'p': 'ppmpp'}, 'ppmpm': {'a': 'pmmam', 'd': 'ppmpd', 'm': 'ppmppm', 'p': 'ppmppm'}, 'pmm': {'a': 'pmma', 'd': 'pmmaaapamd', 'm': 'pappp', 'p': 'pappp'}, 'papppmpdppp': {'a': 'papppmpdp', 'd': 'ppmpdpppd', 'm': 'papppmpdpppm', 'p': 'ppmpdpppp'}, 'papppmpdp': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'papppmpdppa', 'p': 'papppmpdpp'}, 'ppmpdpppmpapp': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'ppmpdpppmpapm', 'p': 'ppmpdpppmpappp'}, 'ppmpppp': {'a': 'papppm', 'd': 'ppmppppd', 'm': 'ppmpppp', 'p': 'ppmpppp'}, 'papppmpdppm': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'pmma', 'p': 'papppmpdppmp'}, 'pap': {'a': 'papppmpa', 'd': 'ppmpd', 'm': 'pappp', 'p': 'ppma'}, 'papppmpdpppm': {'a': 'pmmaa', 'd': 'ppmpdpppd', 'm': 'ppmpdpppmp', 'p': 'papppmpdpppmp'}, 'papppmpdppa': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'papppmpdp'}, 'papppm': {'a': 'papppmpa', 'd': 'ppmpd', 'm': 'papppmp', 'p': 'papppmp'}, 'pa': {'a': 'pmmaa', 'd': 'pmmaaapamd', 'm': 'ppma', 'p': 'pap'}, 'papppmpa': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'papppmpa', 'p': 'ppmpdpppmpa'}, 'papppmpdpppmpa': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'papppmpa', 'p': 'papppmpa'}, 'ppmpd': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'ppmpdp'}, 'ppmpdpppmpap': {'a': 'pmmaaapamd', 'd': 'ppmpd', 'm': 'ppmpdpppmpapm', 'p': 'ppmpdpppmpapp'}, 'papppmpdpppmpm': {'a': 'pmmaa', 'd': 'papppmpd', 'm': 'papppmp', 'p': 'ppmpdpppmpapppp'}, 'pmp': {'a': 'ppma', 'd': 'ppmpd', 'm': 'pappp', 'p': 'ppmp'}, 'ppmpdpppd': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'papppmpdppa'}, 'pmmaaapa': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'pm': {'a': 'pma', 'd': 'pmmaaapamd', 'm': 'pmm', 'p': 'pmp'}, 'ppmppp': {'a': 'papppm', 'd': 'papppmpd', 'm': 'ppmppm', 'p': 'ppmpppp'}, 'papppmpdppmp': {'a': 'pmmaam', 'd': 'ppmpdpppd', 'm': 'ppmpdpppmpappp', 'p': 'papppmpdppmp'}, 'pmmaaapamdp': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}, 'pmmaaap': {'a': 'pmmaaapamd', 'd': 'pmmaaapamd', 'm': 'pmmaaapamd', 'p': 'pmmaaapamd'}}
Explanation matches RNN: 64.39
RNN matches ground truth: 99.98
Explanation matches ground truth: 64.37
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.98
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                                    counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (G (a | (m | p)))))  False          99.94      99.98              64.39                              64.37       401.114718   4.017431e+27         2.057534  [d, , m, p, pamdp, a, ppmmdp, dd, pm, pmpapapm...          0         0         5           68                    100.0                                    99.98               3.473199         True      0.1   0.05
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1105947494506836  s
No positive counterexample found
EQ test took  0.011191368103027344  s
new counterexample: m  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['m']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.10551762580871582  s
EQ test took  0.0015938282012939453  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['m']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x2)
Learning formula with depth 1
learned LTL formula: (~ m)
Learning took:  0.17261242866516113  s
No positive counterexample found
EQ test took  0.04472756385803223  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x3 | x2))
Learning formula with depth 2
learned LTL formula: (~ (m | p))
Learning took:  0.3321402072906494  s
EQ test took  0.04157304763793945  s
new counterexample: pppdp  should be accepted by implementation


positive traces---> 
['', 'pppdp']


negative traces---> 
['m', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
Before normalization: ((x2 | x3) -> (X (x2 | x3)))
Learning formula with depth 3
learned LTL formula: ((m | p) -> (X (m | p)))
Learning took:  0.781930685043335  s
EQ test took  0.08984518051147461  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['', 'pppdp']


negative traces---> 
['m', 'p', 'd']



4  iteration complete



start formula depth: 5
increasing formula depth to  6
increasing formula depth to  7
Before normalization: (((x3 | x1) | x2) -> (X x3))
Learning formula with depth 3
learned LTL formula: ((m | (d | p)) -> (X p))
Learning took:  3.3101673126220703  s
No positive counterexample found
EQ test took  0.19835853576660156  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'pppdp']


negative traces---> 
['m', 'p', 'd', 'a']



5  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((x0 | ((x3 | x1) | x2)) -> (X x3))
Learning formula with depth 4
learned LTL formula: ((a | (m | (d | p))) -> (X p))
Learning took:  69.44174098968506  s
EQ test took  0.05361676216125488  s
new counterexample: pamdp  should be accepted by implementation


positive traces---> 
['', 'pppdp', 'pamdp']


negative traces---> 
['m', 'p', 'd', 'a']



6  iteration complete



start formula depth: 9
Before normalization: (! (x2 | ((x0 | x1) | (G x3))))
Learning formula with depth 4
learned LTL formula: (~ (m | ((a | d) | (G p))))
Learning took:  7.51282811164856  s
EQ test took  0.2965519428253174  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pamdp']


negative traces---> 
['m', 'p', 'd', 'a', 'pd']



7  iteration complete



start formula depth: 9
Before normalization: (((x0 | x3) | (x2 | x1)) -> (X (x0 | x3)))
Learning formula with depth 3
learned LTL formula: (((a | p) | (d | m)) -> (X (a | p)))
Learning took:  10.59097671508789  s
EQ test took  0.20086050033569336  s
new counterexample: pmpmmdp  should be accepted by implementation


positive traces---> 
['', 'pppdp', 'pamdp', 'pmpmmdp']


negative traces---> 
['m', 'p', 'd', 'a', 'pd']



8  iteration complete



start formula depth: 9
Before normalization: ((((x0 | x2) | x3) | x1) -> (X ((x0 | x2) | x3)))
Learning formula with depth 4
learned LTL formula: ((d | (p | (a | m))) -> (X (p | (a | m))))
Learning took:  16.060242891311646  s
EQ test took  0.14726758003234863  s
new counterexample: mm  should be rejected by implementation


positive traces---> 
['', 'pppdp', 'pamdp', 'pmpmmdp']


negative traces---> 
['m', 'p', 'd', 'a', 'pd', 'mm']



9  iteration complete





epsilon= 0.05 delta= 0.5 max_trace_length= 50
query: true
final ltl:  ((d | (p | (a | m))) -> (X (p | (a | m))))
incomplete formula
Number of samples: 153
Number of counterexamples returned: 115
1.9614877945838598e+35 2.206771807881806

Time taken to extract ltl: 401.11426186561584
overall guided extraction time took: 0.020000000000095497
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.01957869529724121
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 62.9
RNN matches ground truth: 99.98
Explanation matches ground truth: 62.88
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query                                 explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                                counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  ((d | (p | (a | m))) -> (X (p | (a | m))))  False          99.94      99.98               62.9                              62.88       401.114262   1.961488e+35         2.206772  [m, , p, pppdp, d, a, pamdp, pd, pmpmmdp, mm]          0         0         4            2                    49.43                                    49.44               0.019579         True     0.05    0.5
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12221050262451172  s
No positive counterexample found
EQ test took  0.014482498168945312  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1088252067565918  s
EQ test took  0.001596212387084961  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.16930413246154785  s
No positive counterexample found
EQ test took  0.054239749908447266  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x1))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.33651161193847656  s
No positive counterexample found
EQ test took  0.1345224380493164  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'm']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x1 | (x2 | x0)))
Learning formula with depth 3
learned LTL formula: (~ (d | (a | m)))
Learning took:  1.0466866493225098  s
No positive counterexample found
EQ test took  0.2100062370300293  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x3 | (x0 | x2)) | x1))
Learning formula with depth 4
learned LTL formula: (~ (d | (p | (a | m))))
Learning took:  7.268007278442383  s
EQ test took  0.343599796295166  s
new counterexample: pmadp  should be accepted by implementation


positive traces---> 
['', 'pmadp']


negative traces---> 
['d', 'a', 'm', 'p']



5  iteration complete



start formula depth: 8
increasing formula depth to  9
Before normalization: ((x3 | (x1 | (x0 | x2))) -> (X x2))
Learning formula with depth 4
learned LTL formula: ((p | (d | (a | m))) -> (X m))
Learning took:  58.929397106170654  s
EQ test took  0.06290960311889648  s
new counterexample: pm  should be rejected by implementation


positive traces---> 
['', 'pmadp']


negative traces---> 
['d', 'a', 'm', 'p', 'pm']



6  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (G ((x2 | x0) | x3))))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (p | (a | m)))))
Learning took:  9.445458889007568  s
No positive counterexample found
EQ test took  0.17452573776245117  s
new counterexample: ad  should be rejected by implementation


positive traces---> 
['', 'pmadp']


negative traces---> 
['d', 'a', 'm', 'p', 'pm', 'ad']



7  iteration complete



start formula depth: 9
Before normalization: (! ((x1 | x0) | (G (x2 | x3))))
Learning formula with depth 4
learned LTL formula: (~ ((a | d) | (G (m | p))))
Learning took:  28.843124389648438  s
No positive counterexample found
EQ test took  0.25482869148254395  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'pmadp']


negative traces---> 
['d', 'a', 'm', 'p', 'pm', 'ad', 'pd']



8  iteration complete



start formula depth: 9
Before normalization: (! (x0 | (G (x1 | (x2 | x3)))))
Learning formula with depth 5
learned LTL formula: (~ (a | (G (d | (m | p)))))
Learning took:  35.96955156326294  s
EQ test took  0.11746740341186523  s
new counterexample: pmmmdp  should be accepted by implementation


positive traces---> 
['', 'pmadp', 'pmmmdp']


negative traces---> 
['d', 'a', 'm', 'p', 'pm', 'ad', 'pd']



9  iteration complete





epsilon= 0.05 delta= 0.25 max_trace_length= 50
query: true
final ltl:  (~ (a | (G (d | (m | p)))))
incomplete formula
Number of samples: 167
Number of counterexamples returned: 103
4.877703784274094e+45 1.7154265331089187

Time taken to extract ltl: 401.11475014686584
overall guided extraction time took: 0.029999999999972715
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.03604769706726074
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 64.46
RNN matches ground truth: 99.98
Explanation matches ground truth: 64.44
Lstar matches RNN: 49.43
Lstar matches ground truth: 49.44
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                            counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (a | (G (d | (m | p)))))  False          99.94      99.98              64.46                              64.44        401.11475   4.877704e+45         1.715427  [d, , a, m, p, pmadp, pm, ad, pd, pmmmdp]          0         0         5            2                    49.43                                    49.44               0.036048         True     0.05   0.25
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1165931224822998  s
No positive counterexample found
EQ test took  0.01724386215209961  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11157560348510742  s
EQ test took  0.001628875732421875  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.1758556365966797  s
No positive counterexample found
EQ test took  0.08136296272277832  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x3 | x1))
Learning formula with depth 2
learned LTL formula: (~ (d | p))
Learning took:  0.3286774158477783  s
No positive counterexample found
EQ test took  0.15173053741455078  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'p', 'a']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! ((x3 | x0) | x1))
Learning formula with depth 3
learned LTL formula: (~ (d | (a | p)))
Learning took:  0.9855313301086426  s
EQ test took  0.05169963836669922  s
new counterexample: papdp  should be accepted by implementation


positive traces---> 
['', 'papdp']


negative traces---> 
['d', 'p', 'a']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
Before normalization: (((x3 | x1) | x0) -> (X x0))
Learning formula with depth 3
learned LTL formula: ((a | (d | p)) -> (X a))
Learning took:  2.756497383117676  s
EQ test took  0.05819249153137207  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['', 'papdp']


negative traces---> 
['d', 'p', 'a', 'm']



5  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((x3 | ((x2 | x0) | x1)) -> (X x0))
Learning formula with depth 4
learned LTL formula: ((p | (d | (a | m))) -> (X a))
Learning took:  66.6371717453003  s
No positive counterexample found
EQ test took  0.35645413398742676  s
new counterexample: da  should be rejected by implementation


positive traces---> 
['', 'papdp']


negative traces---> 
['d', 'p', 'a', 'm', 'da']



6  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (x0 | (G (x3 | x2)))))
Learning formula with depth 5
learned LTL formula: (~ (d | (a | (G (m | p)))))
Learning took:  12.398538589477539  s
No positive counterexample found
EQ test took  0.2536332607269287  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'papdp']


negative traces---> 
['d', 'p', 'a', 'm', 'da', 'pd']



7  iteration complete





epsilon= 0.05 delta= 0.1 max_trace_length= 50
query: true
final ltl:  (~ (d | (a | (G (m | p)))))
incomplete formula
Number of samples: 157
Number of counterexamples returned: 80
2.402174361395841e+44 1.4070480973486386

Time taken to extract ltl: 401.114009141922
overall guided extraction time took: 0.6000000000000227
generated counterexamples were: (format: (counterexample, counterexample generation time))
('pampdp', 0.09000000000003183)
('pmpdp', 0.11000000000001364)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.6075279712677002
number of states of the dfa: 22
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'p'}, 'pmpdppp': {'a': 'pama', 'd': 'pmpd', 'm': 'pamp', 'p': 'pampdpppp'}, 'pampd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pampdp'}, 'pampdppp': {'a': 'pampdp', 'd': 'pmpdp', 'm': 'pampdppp', 'p': 'pampdpppp'}, 'pampdppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pampdppd'}, 'pam': {'a': 'pama', 'd': 'pmpd', 'm': 'pamp', 'p': 'pamp'}, 'p': {'a': 'pa', 'd': 'a', 'm': 'pm', 'p': 'pa'}, 'pmpdp': {'a': 'a', 'd': 'a', 'm': 'pmpdpm', 'p': 'pmpdpp'}, 'pama': {'a': 'a', 'd': 'pmpd', 'm': 'pama', 'p': 'pama'}, 'pmpdpp': {'a': 'pmpdppa', 'd': 'a', 'm': 'pama', 'p': 'pmpdppp'}, 'paa': {'a': 'a', 'd': 'a', 'm': 'pama', 'p': 'pama'}, 'pmpd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pmpdp'}, 'pampdpppp': {'a': 'pampdpp', 'd': 'pampdp', 'm': 'pampdppp', 'p': 'pampdpppp'}, 'pmpdppa': {'a': 'a', 'd': 'a', 'm': 'pmpdppa', 'p': 'pama'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'pampdpp': {'a': '', 'd': 'pampdppd', 'm': 'pampdpp', 'p': 'pampdppp'}, 'pampdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pampdpp'}, 'pa': {'a': 'paa', 'd': 'a', 'm': 'pam', 'p': 'pam'}, 'pmp': {'a': 'pama', 'd': 'pmpd', 'm': 'pamp', 'p': 'pamp'}, 'pmpdpm': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'paa'}, 'pm': {'a': 'pam', 'd': 'a', 'm': 'pa', 'p': 'pam'}, 'pamp': {'a': 'pama', 'd': 'pampd', 'm': 'pamp', 'p': 'pamp'}}
Explanation matches RNN: 75.49
RNN matches ground truth: 99.98
Explanation matches ground truth: 75.47
Lstar matches RNN: 99.98
Lstar matches ground truth: 99.97
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (d | (a | (G (m | p)))))  False          99.94      99.98              75.49                              75.47       401.114009   2.402174e+44         1.407048  [d, , p, a, papdp, m, da, pd]          0         0         5           22                    99.98                                    99.97               0.607528         True     0.05    0.1
target: email match
query: true


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10186457633972168  s
No positive counterexample found
EQ test took  0.010461568832397461  s
new counterexample: m  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['m']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1091468334197998  s
EQ test took  0.0016243457794189453  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['m']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x2)
Learning formula with depth 1
learned LTL formula: (~ m)
Learning took:  0.17008566856384277  s
No positive counterexample found
EQ test took  0.1001136302947998  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x3 | x2))
Learning formula with depth 2
learned LTL formula: (~ (m | p))
Learning took:  0.35553503036499023  s
EQ test took  0.05148506164550781  s
new counterexample: ppmdp  should be accepted by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['m', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
Before normalization: ((x2 | x3) -> (X x3))
Learning formula with depth 2
learned LTL formula: ((m | p) -> (X p))
Learning took:  0.7905890941619873  s
EQ test took  0.07205939292907715  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['m', 'p', 'd']



4  iteration complete



start formula depth: 5
increasing formula depth to  6
increasing formula depth to  7
Before normalization: (((x2 | x1) | x3) -> (X x3))
Learning formula with depth 3
learned LTL formula: ((p | (d | m)) -> (X p))
Learning took:  3.419606924057007  s
EQ test took  0.15032482147216797  s
new counterexample: pmmpadp  should be accepted by implementation


positive traces---> 
['', 'ppmdp', 'pmmpadp']


negative traces---> 
['m', 'p', 'd']



5  iteration complete



start formula depth: 7
Before normalization: ((x1 | (x2 | x3)) -> (X (x2 | x3)))
Learning formula with depth 3
learned LTL formula: ((d | (m | p)) -> (X (m | p)))
Learning took:  2.974867582321167  s
EQ test took  0.10102438926696777  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'ppmdp', 'pmmpadp']


negative traces---> 
['m', 'p', 'd', 'a']



6  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: ((x1 | (x0 | (x2 | x3))) -> (X (x2 | x3)))
Learning formula with depth 4
learned LTL formula: ((d | (a | (m | p))) -> (X (m | p)))
Learning took:  106.83742070198059  s
No positive counterexample found
EQ test took  0.3222520351409912  s
new counterexample: dm  should be rejected by implementation


positive traces---> 
['', 'ppmdp', 'pmmpadp']


negative traces---> 
['m', 'p', 'd', 'a', 'dm']



7  iteration complete



start formula depth: 9
Before normalization: (! (x2 | ((x0 | (G x3)) | x1)))
Learning formula with depth 5
learned LTL formula: (~ (m | (d | (a | (G p)))))
Learning took:  7.523529767990112  s
No positive counterexample found
EQ test took  0.4299039840698242  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'ppmdp', 'pmmpadp']


negative traces---> 
['m', 'p', 'd', 'a', 'dm', 'pd']



8  iteration complete





epsilon= 0.05 delta= 0.05 max_trace_length= 50
query: true
final ltl:  (~ (m | (d | (a | (G p)))))
incomplete formula
Number of samples: 185
Number of counterexamples returned: 50
5.898896994143947e+42 0.8016968263895787

Time taken to extract ltl: 401.1146593093872
overall guided extraction time took: 1.759999999999991
generated counterexamples were: (format: (counterexample, counterexample generation time))
('pamapmppmdp', 0.2300000000000182)
('papapppdppa', 0.2599999999999909)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 1.7466564178466797
number of states of the dfa: 39
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'p'}, 'ppp': {'a': 'pam', 'd': 'pamapmppmd', 'm': 'ppp', 'p': 'pppp'}, 'pamapmppmd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamapmppmdp'}, 'papapppdppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papapppdppd'}, 'papapppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papapppdp'}, 'papapppdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papapppdpp'}, 'papapppm': {'a': 'a', 'd': 'pamapmppmd', 'm': 'papapppm', 'p': 'papapp'}, 'papapppdpppm': {'a': 'a', 'd': 'papapppd', 'm': 'papappp', 'p': 'papapppdpppm'}, 'pamapmppm': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'papa'}, 'papa': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'pamapmpp'}, 'pam': {'a': 'papa', 'd': 'pamapmppmd', 'm': 'pamm', 'p': 'pamm'}, 'p': {'a': 'pa', 'd': 'a', 'm': 'pm', 'p': 'pp'}, 'papapppdppa': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'pma': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pam', 'p': 'pamm'}, 'pama': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'pamapmpp'}, 'pamapmp': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'pamapmpp'}, 'papapppdppp': {'a': 'papapppdppa', 'd': 'papapppdpppd', 'm': 'papapppdpppm', 'p': 'papapppdppp'}, 'pmmam': {'a': 'a', 'd': 'papapppd', 'm': 'pamm', 'p': 'pamm'}, 'pppp': {'a': 'pamm', 'd': 'papapppd', 'm': 'pppp', 'p': 'pppp'}, 'paa': {'a': 'a', 'd': 'a', 'm': 'pamapmppm', 'p': 'pamapmpp'}, 'papapp': {'a': 'a', 'd': 'papapppd', 'm': 'papa', 'p': 'papappp'}, 'pp': {'a': 'pam', 'd': 'a', 'm': 'pmp', 'p': 'ppp'}, 'papappp': {'a': 'a', 'd': 'papapppd', 'm': 'papapppm', 'p': 'papappp'}, 'papapppdpppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'papapppdp'}, 'pamapmpp': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'papapp'}, 'papap': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'papapp'}, 'pmma': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pmmam', 'p': 'pamm'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'papapppdpp': {'a': 'papapppdppa', 'd': 'papapppdppd', 'm': 'papapppdppd', 'p': 'papapppdppp'}, 'pamap': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'papapp'}, 'pmm': {'a': 'pmma', 'd': 'a', 'm': 'pam', 'p': 'pam'}, 'pap': {'a': 'papa', 'd': 'pamapmppmd', 'm': 'pamm', 'p': 'pamm'}, 'pamapm': {'a': 'a', 'd': 'pamapmppmd', 'm': 'pamapmppm', 'p': 'papa'}, 'pa': {'a': 'paa', 'd': 'a', 'm': 'pam', 'p': 'pam'}, 'pamapmppmdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pamapmppmdpp'}, 'pmp': {'a': 'pam', 'd': 'pamapmppmd', 'm': 'pam', 'p': 'ppp'}, 'pm': {'a': 'pma', 'd': 'a', 'm': 'pmm', 'p': 'pmp'}, 'pamm': {'a': 'pamapmpp', 'd': 'papapppd', 'm': 'pamm', 'p': 'pamm'}, 'pamapmppmdpp': {'a': 'a', 'd': 'pamapmppmd', 'm': 'papapppdppd', 'p': 'papapppdppp'}}
Explanation matches RNN: 87.63
RNN matches ground truth: 99.98
Explanation matches ground truth: 87.61
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.98
target query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                         counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  true  (~ (m | (d | (a | (G p)))))  False          99.94      99.98              87.63                              87.61       401.114659   5.898897e+42         0.801697  [m, , p, ppmdp, d, pmmpadp, a, dm, pd]          0         0         5           39                    100.0                                    99.98               1.746656         True     0.05   0.05
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.13118338584899902  s
EQ test took  0.0006630420684814453  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12596583366394043  s
EQ test took  0.023003101348876953  s


epsilon= 0.5 delta= 0.5 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.2842400074005127
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.016762256622314453
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0          0.28424          None            None              []          0         0         0            2                    100.0                                    100.0               0.016762         True      0.5    0.5
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12180685997009277  s
EQ test took  0.0006811618804931641  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11189126968383789  s
EQ test took  0.02315211296081543  s


epsilon= 0.5 delta= 0.25 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.26139187812805176
overall guided extraction time took: 0.04000000000007731
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.0366063117980957
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.261392          None            None              []          0         0         0            2                    100.0                                    100.0               0.036606         True      0.5   0.25
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11858034133911133  s
EQ test took  0.0006830692291259766  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11959195137023926  s
EQ test took  0.027289390563964844  s


epsilon= 0.5 delta= 0.1 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.2699739933013916
overall guided extraction time took: 0.11000000000001364
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.10962724685668945
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.269974          None            None              []          0         0         0            2                    100.0                                    100.0               0.109627         True      0.5    0.1
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1201028823852539  s
EQ test took  0.0007224082946777344  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11567139625549316  s
EQ test took  0.032398223876953125  s


epsilon= 0.5 delta= 0.05 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.2727792263031006
overall guided extraction time took: 0.2699999999999818
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.2664511203765869
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.272779          None            None              []          0         0         0            2                    100.0                                    100.0               0.266451         True      0.5   0.05
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1150200366973877  s
EQ test took  0.0006606578826904297  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12837982177734375  s
EQ test took  0.033545732498168945  s


epsilon= 0.25 delta= 0.5 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.2813386917114258
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.016259193420410156
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.281339          None            None              []          0         0         0            2                    100.0                                    100.0               0.016259         True     0.25    0.5
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11313748359680176  s
EQ test took  0.0006573200225830078  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11623477935791016  s
EQ test took  0.040879249572753906  s


epsilon= 0.25 delta= 0.25 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.2746760845184326
overall guided extraction time took: 0.03999999999996362
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.04368448257446289
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.274676          None            None              []          0         0         0            2                    100.0                                    100.0               0.043684         True     0.25   0.25
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12417912483215332  s
EQ test took  0.00067138671875  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12145233154296875  s
EQ test took  0.04953360557556152  s


epsilon= 0.25 delta= 0.1 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.299668550491333
overall guided extraction time took: 0.11000000000012733
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.11001729965209961
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.299669          None            None              []          0         0         0            2                    100.0                                    100.0               0.110017         True     0.25    0.1
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1109778881072998  s
EQ test took  0.000644683837890625  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1063234806060791  s
EQ test took  0.06544733047485352  s


epsilon= 0.25 delta= 0.05 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.28653717041015625
overall guided extraction time took: 0.2599999999999909
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.2601468563079834
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.286537          None            None              []          0         0         0            2                    100.0                                    100.0               0.260147         True     0.25   0.05
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11832118034362793  s
EQ test took  0.0006530284881591797  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11933231353759766  s
EQ test took  0.0656132698059082  s


epsilon= 0.1 delta= 0.5 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.3077876567840576
overall guided extraction time took: 0.01999999999998181
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.01615142822265625
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.307788          None            None              []          0         0         0            2                    100.0                                    100.0               0.016151         True      0.1    0.5
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12313008308410645  s
EQ test took  0.0006594657897949219  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11575055122375488  s
EQ test took  0.09334659576416016  s


epsilon= 0.1 delta= 0.25 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.33666396141052246
overall guided extraction time took: 0.03999999999996362
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.03796815872192383
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.336664          None            None              []          0         0         0            2                    100.0                                    100.0               0.037968         True      0.1   0.25
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1138148307800293  s
EQ test took  0.000705718994140625  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12917089462280273  s
EQ test took  0.1166529655456543  s


epsilon= 0.1 delta= 0.1 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.3636941909790039
overall guided extraction time took: 0.10999999999989996
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.11284542083740234
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.363694          None            None              []          0         0         0            2                    100.0                                    100.0               0.112845         True      0.1    0.1
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.10829591751098633  s
EQ test took  0.000637054443359375  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12746858596801758  s
EQ test took  0.14275264739990234  s


epsilon= 0.1 delta= 0.05 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.3823251724243164
overall guided extraction time took: 0.2599999999999909
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.2591986656188965
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.382325          None            None              []          0         0         0            2                    100.0                                    100.0               0.259199         True      0.1   0.05
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11594605445861816  s
EQ test took  0.0006654262542724609  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11819624900817871  s
EQ test took  0.13539958000183105  s


epsilon= 0.05 delta= 0.5 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.37399744987487793
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.016167402267456055
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.373997          None            None              []          0         0         0            2                    100.0                                    100.0               0.016167         True     0.05    0.5
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11206889152526855  s
EQ test took  0.0006642341613769531  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1165924072265625  s
EQ test took  0.17872071266174316  s


epsilon= 0.05 delta= 0.25 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.4117729663848877
overall guided extraction time took: 0.029999999999972715
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.03687119483947754
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0         0.411773          None            None              []          0         0         0            2                    100.0                                    100.0               0.036871         True     0.05   0.25
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11100196838378906  s
EQ test took  0.0006570816040039062  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1232151985168457  s
EQ test took  0.23252224922180176  s


epsilon= 0.05 delta= 0.1 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.47120046615600586
overall guided extraction time took: 0.11000000000012733
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.1131284236907959
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0           0.4712          None            None              []          0         0         0            2                    100.0                                    100.0               0.113128         True     0.05    0.1
target: email match
query: (m)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12896728515625  s
EQ test took  0.0006735324859619141  s
new counterexample:   should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12830018997192383  s
EQ test took  0.2710914611816406  s


epsilon= 0.05 delta= 0.05 max_trace_length= 50
query: (m)
final ltl:  false

Time taken to extract ltl: 0.5329000949859619
overall guided extraction time took: 0.2699999999999818
generated counterexamples were: (format: (counterexample, counterexample generation time))
('', 0.0)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.26709938049316406
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 100.0
Explanation matches ground truth: 100.0
Lstar matches RNN: 100.0
Lstar matches ground truth: 100.0
target query explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match   (m)       false   True          99.94      100.0              100.0                              100.0           0.5329          None            None              []          0         0         0            2                    100.0                                    100.0               0.267099         True     0.05   0.05
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11536574363708496  s
No positive counterexample found
EQ test took  0.003194570541381836  s
new counterexample: m  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['m']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1193234920501709  s
EQ test took  0.0016756057739257812  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['m']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x2)
Learning formula with depth 1
learned LTL formula: (~ m)
Learning took:  0.1679065227508545  s
No positive counterexample found
EQ test took  0.013664007186889648  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x3 | x2))
Learning formula with depth 2
learned LTL formula: (~ (m | p))
Learning took:  0.3426074981689453  s
No positive counterexample found
EQ test took  0.015453815460205078  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p', 'd']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x2 | (x3 | x1)))
Learning formula with depth 3
learned LTL formula: (~ (m | (d | p)))
Learning took:  0.99605393409729  s
No positive counterexample found
EQ test took  0.023004770278930664  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p', 'd', 'a']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x0 | x2) | (x1 | x3)))
Learning formula with depth 3
learned LTL formula: (~ ((a | m) | (d | p)))
Learning took:  8.2353196144104  s
EQ test took  0.039697885513305664  s


epsilon= 0.5 delta= 0.5 max_trace_length= 50
query: ~F(a)
final ltl:  (~ ((a | m) | (d | p)))

Time taken to extract ltl: 10.082722663879395
overall guided extraction time took: 0.010000000000218279
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.01044774055480957
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 99.66
Explanation matches ground truth: 99.66
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ ((a | m) | (d | p)))   True          99.94      99.66              100.0                              99.66        10.082723          None            None  [m, , p, d, a]          0         0         3            2                    100.0                                    99.66               0.010448         True      0.5    0.5
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11719489097595215  s
No positive counterexample found
EQ test took  0.0037779808044433594  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11279487609863281  s
EQ test took  0.0017249584197998047  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.18651056289672852  s
No positive counterexample found
EQ test took  0.013039588928222656  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x1))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.33670473098754883  s
No positive counterexample found
EQ test took  0.015530824661254883  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x1 | (x0 | x3)))
Learning formula with depth 3
learned LTL formula: (~ (d | (a | p)))
Learning took:  1.0387015342712402  s
No positive counterexample found
EQ test took  0.03519773483276367  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'a', 'p', 'm']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! (x1 | (x0 | (x2 | x3))))
Learning formula with depth 4
learned LTL formula: (~ (d | (a | (m | p))))
Learning took:  8.307813882827759  s
EQ test took  0.04687070846557617  s


epsilon= 0.5 delta= 0.25 max_trace_length= 50
query: ~F(a)
final ltl:  (~ (d | (a | (m | p))))

Time taken to extract ltl: 10.224998235702515
overall guided extraction time took: 0.029999999999972715
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.027948379516601562
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 99.66
Explanation matches ground truth: 99.66
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ (d | (a | (m | p))))   True          99.94      99.66              100.0                              99.66        10.224998          None            None  [d, , a, p, m]          0         0         4            2                    100.0                                    99.66               0.027948         True      0.5   0.25
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12347221374511719  s
No positive counterexample found
EQ test took  0.0041658878326416016  s
new counterexample: a  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['a']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11615443229675293  s
EQ test took  0.0017633438110351562  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['a']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x0)
Learning formula with depth 1
learned LTL formula: (~ a)
Learning took:  0.18850994110107422  s
No positive counterexample found
EQ test took  0.009111404418945312  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x0 | x2))
Learning formula with depth 2
learned LTL formula: (~ (a | m))
Learning took:  0.3488953113555908  s
No positive counterexample found
EQ test took  0.018453598022460938  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'm', 'd']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x0 | (x1 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (a | (d | m)))
Learning took:  1.0043854713439941  s
No positive counterexample found
EQ test took  0.03339695930480957  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'm', 'd', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! (x1 | (x0 | (x2 | x3))))
Learning formula with depth 4
learned LTL formula: (~ (d | (a | (m | p))))
Learning took:  8.380080699920654  s
EQ test took  0.049945831298828125  s


epsilon= 0.5 delta= 0.1 max_trace_length= 50
query: ~F(a)
final ltl:  (~ (d | (a | (m | p))))

Time taken to extract ltl: 10.287529230117798
overall guided extraction time took: 0.25
generated counterexamples were: (format: (counterexample, counterexample generation time))
('pppdp', 0.07999999999992724)
<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.2500767707824707
number of states of the dfa: 7
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'p'}, 'p': {'a': 'p', 'd': 'a', 'm': 'pp', 'p': 'pp'}, 'ppp': {'a': 'ppp', 'd': 'pppd', 'm': 'ppp', 'p': 'ppp'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'pppd': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pppdp'}, 'pppdp': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'pppdp'}, 'pp': {'a': 'ppp', 'd': 'a', 'm': 'ppp', 'p': 'ppp'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 99.66
Explanation matches ground truth: 99.66
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ (d | (a | (m | p))))   True          99.94      99.66              100.0                              99.66        10.287529          None            None  [a, , m, d, p]          0         0         4            7                    100.0                                    99.66               0.250077         True      0.5    0.1
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.1323108673095703  s
No positive counterexample found
EQ test took  0.004873514175415039  s
new counterexample: a  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['a']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12645316123962402  s
EQ test took  0.0016562938690185547  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['a']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x0)
Learning formula with depth 1
learned LTL formula: (~ a)
Learning took:  0.1879560947418213  s
No positive counterexample found
EQ test took  0.011928319931030273  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x1 | x0))
Learning formula with depth 2
learned LTL formula: (~ (a | d))
Learning took:  0.35471487045288086  s
No positive counterexample found
EQ test took  0.028276443481445312  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd', 'm']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! ((x0 | x1) | x2))
Learning formula with depth 3
learned LTL formula: (~ (m | (a | d)))
Learning took:  1.0733013153076172  s
No positive counterexample found
EQ test took  0.04008626937866211  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['a', 'd', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x1 | (x3 | x2)) | x0))
Learning formula with depth 4
learned LTL formula: (~ (a | (d | (m | p))))
Learning took:  7.7666285037994385  s
EQ test took  0.05586695671081543  s


epsilon= 0.5 delta= 0.05 max_trace_length= 50
query: ~F(a)
final ltl:  (~ (a | (d | (m | p))))

Time taken to extract ltl: 9.793380975723267
overall guided extraction time took: 0.22000000000002728
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.21983575820922852
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 99.66
Explanation matches ground truth: 99.66
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ (a | (d | (m | p))))   True          99.94      99.66              100.0                              99.66         9.793381          None            None  [a, , d, m, p]          0         0         4            2                    100.0                                    99.66               0.219836         True      0.5   0.05
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.12257599830627441  s
No positive counterexample found
EQ test took  0.004110097885131836  s
new counterexample: p  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['p']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12441039085388184  s
EQ test took  0.001638174057006836  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['p']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x3)
Learning formula with depth 1
learned LTL formula: (~ p)
Learning took:  0.1841421127319336  s
No positive counterexample found
EQ test took  0.022217750549316406  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['p', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x2 | x3))
Learning formula with depth 2
learned LTL formula: (~ (m | p))
Learning took:  0.3596913814544678  s
No positive counterexample found
EQ test took  0.04039645195007324  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['p', 'm', 'd']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! ((x2 | x3) | x1))
Learning formula with depth 3
learned LTL formula: (~ (d | (m | p)))
Learning took:  1.1094484329223633  s
No positive counterexample found
EQ test took  0.051817893981933594  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['p', 'm', 'd', 'a']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! (((x1 | x0) | x3) | x2))
Learning formula with depth 4
learned LTL formula: (~ (m | (p | (a | d))))
Learning took:  8.669555425643921  s
EQ test took  0.07317781448364258  s


epsilon= 0.25 delta= 0.5 max_trace_length= 50
query: ~F(a)
final ltl:  (~ (m | (p | (a | d))))

Time taken to extract ltl: 10.773039817810059
overall guided extraction time took: 0.009999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.010304689407348633
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 99.66
Explanation matches ground truth: 99.66
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ (m | (p | (a | d))))   True          99.94      99.66              100.0                              99.66         10.77304          None            None  [p, , m, d, a]          0         0         4            2                    100.0                                    99.66               0.010305         True     0.25    0.5
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11254143714904785  s
No positive counterexample found
EQ test took  0.0052340030670166016  s
new counterexample: m  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['m']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.11530637741088867  s
EQ test took  0.0016715526580810547  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['m']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x2)
Learning formula with depth 1
learned LTL formula: (~ m)
Learning took:  0.1670374870300293  s
No positive counterexample found
EQ test took  0.019357919692993164  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x3 | x2))
Learning formula with depth 2
learned LTL formula: (~ (m | p))
Learning took:  0.33915042877197266  s
No positive counterexample found
EQ test took  0.03088665008544922  s
new counterexample: d  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p', 'd']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x2 | (x3 | x1)))
Learning formula with depth 3
learned LTL formula: (~ (m | (d | p)))
Learning took:  0.999718189239502  s
No positive counterexample found
EQ test took  0.05327916145324707  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['m', 'p', 'd', 'a']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x0 | x2) | (x1 | x3)))
Learning formula with depth 3
learned LTL formula: (~ ((a | m) | (d | p)))
Learning took:  7.796692132949829  s
EQ test took  0.0777885913848877  s


epsilon= 0.25 delta= 0.25 max_trace_length= 50
query: ~F(a)
final ltl:  (~ ((a | m) | (d | p)))

Time taken to extract ltl: 9.727956771850586
overall guided extraction time took: 0.03000000000020009
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.028191566467285156
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 100.0
RNN matches ground truth: 99.66
Explanation matches ground truth: 99.66
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query              explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time revised delta revised epsilon counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ ((a | m) | (d | p)))   True          99.94      99.66              100.0                              99.66         9.727957          None            None  [m, , p, d, a]          0         0         3            2                    100.0                                    99.66               0.028192         True     0.25   0.25
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11345267295837402  s
No positive counterexample found
EQ test took  0.006432294845581055  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.12928557395935059  s
EQ test took  0.0017697811126708984  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.1690831184387207  s
No positive counterexample found
EQ test took  0.03438735008239746  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x2 | x1))
Learning formula with depth 2
learned LTL formula: (~ (d | m))
Learning took:  0.32984137535095215  s
No positive counterexample found
EQ test took  0.04613614082336426  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'a']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! ((x1 | x0) | x2))
Learning formula with depth 3
learned LTL formula: (~ (m | (a | d)))
Learning took:  1.0665361881256104  s
No positive counterexample found
EQ test took  0.08516359329223633  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'a', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
increasing formula depth to  8
Before normalization: (! ((x0 | (x1 | x2)) | x3))
Learning formula with depth 4
learned LTL formula: (~ (p | (a | (d | m))))
Learning took:  9.374595403671265  s
EQ test took  0.08978056907653809  s
new counterexample: ppmdp  should be accepted by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['d', 'm', 'a', 'p']



5  iteration complete



start formula depth: 8
increasing formula depth to  9
Before normalization: (((x2 | (x1 | x3)) | x0) -> (X x3))
Learning formula with depth 4
learned LTL formula: ((a | (m | (d | p))) -> (X p))
Learning took:  57.12384223937988  s
EQ test took  0.08568215370178223  s
new counterexample: dp  should be rejected by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['d', 'm', 'a', 'p', 'dp']



6  iteration complete



start formula depth: 9
Before normalization: (! (((x2 | x1) | x0) | (G x3)))
Learning formula with depth 4
learned LTL formula: (~ ((G p) | (a | (d | m))))
Learning took:  5.268538951873779  s
No positive counterexample found
EQ test took  0.09542083740234375  s
new counterexample: pa  should be rejected by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['d', 'm', 'a', 'p', 'dp', 'pa']



7  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (G (x0 | (x3 | x2)))))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (a | (m | p)))))
Learning took:  12.742274284362793  s
No positive counterexample found
EQ test took  0.044310569763183594  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['d', 'm', 'a', 'p', 'dp', 'pa', 'pd']



8  iteration complete



start formula depth: 9
Before normalization: (! (x2 | (G (x3 | (x0 | x1)))))
Learning formula with depth 5
learned LTL formula: (~ (m | (G (p | (a | d)))))
Learning took:  51.96707582473755  s
No positive counterexample found
EQ test took  0.05325484275817871  s
new counterexample: dm  should be rejected by implementation


positive traces---> 
['', 'ppmdp']


negative traces---> 
['d', 'm', 'a', 'p', 'dp', 'pa', 'pd', 'dm']



9  iteration complete





epsilon= 0.25 delta= 0.1 max_trace_length= 50
query: ~F(a)
final ltl:  (~ (m | (G (p | (a | d)))))
incomplete formula
Number of samples: 37
Number of counterexamples returned: 27
28592678.734222483 2.1971246347699624

Time taken to extract ltl: 401.1148040294647
overall guided extraction time took: 0.09999999999990905
generated counterexamples were: (format: (counterexample, counterexample generation time))

<IPython.core.display.Image object>

Time taken to extract lstar-dfa: 0.0972285270690918
number of states of the dfa: 2
returned flag: True
transitions:->
{'': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}, 'a': {'a': 'a', 'd': 'a', 'm': 'a', 'p': 'a'}}
Explanation matches RNN: 44.66
RNN matches ground truth: 99.66
Explanation matches ground truth: 44.33
Lstar matches RNN: 100.0
Lstar matches ground truth: 99.66
target  query                  explanation status  test accuracy  rnn score  explanation score  explanation score on ground truth  extraction time  revised delta  revised epsilon                        counterexamples train size test size ltl_depth lstar_states  lstar explanation score  lstar explanation score on ground truth  lstar extraction time lstar_status  epsilon  delta
email match  ~F(a)  (~ (m | (G (p | (a | d)))))  False          99.94      99.66              44.66                              44.33       401.114804   2.859268e+07         2.197125  [d, , m, a, p, ppmdp, dp, pa, pd, dm]          0         0         5            2                    100.0                                    99.66               0.097229         True     0.25    0.1
target: email match
query: ~F(a)


positive traces---> 
[]


negative traces---> 
[]



Learning formula with depth 0
learned LTL formula: true
Learning took:  0.11647367477416992  s
No positive counterexample found
EQ test took  0.007215023040771484  s
new counterexample: d  should be rejected by implementation


positive traces---> 
[]


negative traces---> 
['d']



0  iteration complete



Learning formula with depth 0
learned LTL formula: false
Learning took:  0.1203622817993164  s
EQ test took  0.0017108917236328125  s
new counterexample:   should be accepted by implementation


positive traces---> 
['']


negative traces---> 
['d']



1  iteration complete



start formula depth: 1
increasing formula depth to  2
Before normalization: (! x1)
Learning formula with depth 1
learned LTL formula: (~ d)
Learning took:  0.17682623863220215  s
No positive counterexample found
EQ test took  0.03038191795349121  s
new counterexample: m  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm']



2  iteration complete



start formula depth: 2
increasing formula depth to  3
increasing formula depth to  4
Before normalization: (! (x2 | x1))
Learning formula with depth 2
learned LTL formula: (~ (d | m))
Learning took:  0.32864975929260254  s
No positive counterexample found
EQ test took  0.05285477638244629  s
new counterexample: p  should be rejected by implementation


positive traces---> 
['']


negative traces---> 
['d', 'm', 'p']



3  iteration complete



start formula depth: 4
increasing formula depth to  5
increasing formula depth to  6
Before normalization: (! (x3 | (x1 | x2)))
Learning formula with depth 3
learned LTL formula: (~ (p | (d | m)))
Learning took:  1.0851750373840332  s
EQ test took  0.0217282772064209  s
new counterexample: ppmpdp  should be accepted by implementation


positive traces---> 
['', 'ppmpdp']


negative traces---> 
['d', 'm', 'p']



4  iteration complete



start formula depth: 6
increasing formula depth to  7
Before normalization: ((x1 | (x3 | x2)) -> (X x3))
Learning formula with depth 3
learned LTL formula: ((d | (m | p)) -> (X p))
Learning took:  3.1252596378326416  s
EQ test took  0.0641939640045166  s
new counterexample: a  should be rejected by implementation


positive traces---> 
['', 'ppmpdp']


negative traces---> 
['d', 'm', 'p', 'a']



5  iteration complete



start formula depth: 7
increasing formula depth to  8
increasing formula depth to  9
Before normalization: (! ((x1 | (x2 | x0)) | (G x3)))
Learning formula with depth 4
learned LTL formula: (~ ((G p) | (d | (a | m))))
Learning took:  75.1038727760315  s
No positive counterexample found
EQ test took  0.08672046661376953  s
new counterexample: pa  should be rejected by implementation


positive traces---> 
['', 'ppmpdp']


negative traces---> 
['d', 'm', 'p', 'a', 'pa']



6  iteration complete



start formula depth: 9
Before normalization: (((x1 | x2) | (x3 | x0)) -> (X x3))
Learning formula with depth 3
learned LTL formula: (((a | p) | (d | m)) -> (X p))
Learning took:  7.4644153118133545  s
No positive counterexample found
EQ test took  0.09588432312011719  s
new counterexample: mp  should be rejected by implementation


positive traces---> 
['', 'ppmpdp']


negative traces---> 
['d', 'm', 'p', 'a', 'pa', 'mp']



7  iteration complete



start formula depth: 9
Before normalization: (! (x1 | (G ((x3 | x2) | x0))))
Learning formula with depth 5
learned LTL formula: (~ (d | (G (a | (m | p)))))
Learning took:  18.05764675140381  s
No positive counterexample found
EQ test took  0.0500028133392334  s
new counterexample: pd  should be rejected by implementation


positive traces---> 
['', 'ppmpdp']


negative traces---> 
['d', 'm', 'p', 'a', 'pa', 'mp', 'pd']



8  iteration complete



start formula depth: 9
Before normalization: (! (x2 | (G (x3 | (x1 | x0)))))
Learning formula with depth 5
learned LTL formula: (~ (m | (G (p | (a | d)))))
Learning took:  41.349427461624146  s
No positive counterexample found
EQ test took  0.04952692985534668  s
new counterexample: dm  should be rejected by implementation


positive traces---> 
['', 'ppmpdp']


negative traces---> 
['d', 'm', 'p', 'a', 'pa', 'mp', 'pd', 'dm']



9  iteration complete





epsilon= 0.25 delta= 0.05 max_trace_length= 50
query: ~F(a)
final ltl:  (~ (m | (G (p | (a | d)))))
incomplete formula
Number of samples: 40
Number of counterexamples returned: 32
10407917.32669748 2.6442262036813875

Time taken to extract ltl: 401.1129615306854
Interrupted due to time limit
